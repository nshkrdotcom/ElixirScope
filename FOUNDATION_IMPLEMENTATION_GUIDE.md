# ElixirScope Missing Functionality Implementation Guide

This document provides detailed technical specifications for implementing the missing core functionality using Test-Driven Development (TDD). Each component includes comprehensive test requirements and implementation details to ensure real functionality, not just passing facades.

## Table of Contents
1. [AST Transformation Engine](#1-ast-transformation-engine)
2. [Phoenix Integration Layer](#2-phoenix-integration-layer)  
3. [AI Code Analysis Engine](#3-ai-code-analysis-engine)
4. [Distributed System Support](#4-distributed-system-support)
5. [Production Integration Tests](#5-production-integration-tests)

---

## 1. AST Transformation Engine

### Overview
The AST Transformation Engine is the critical missing piece that transforms Elixir source code at compile time to inject ElixirScope instrumentation calls. This must be implemented as a Mix compiler that runs before the standard Elixir compiler.

### 1.1 Mix Compiler Integration

#### Test Specification: `test/elixir_scope/compiler/mix_task_test.exs`

```elixir
defmodule ElixirScope.Compiler.MixTaskTest do
  use ExUnit.Case
  import ExUnit.CaptureIO
  alias ElixirScope.Compiler.MixTask

  @test_project_path "test/fixtures/sample_project"
  @instrumented_output_path "test/fixtures/instrumented_output"

  describe "Mix compiler integration" do
    test "registers as a Mix compiler" do
      compilers = Mix.compilers()
      assert :elixir_scope in compilers
    end

    test "runs before elixir compiler" do
      # Verify elixir_scope runs before :elixir in the compiler chain
      position_elixir_scope = Enum.find_index(Mix.compilers(), &(&1 == :elixir_scope))
      position_elixir = Enum.find_index(Mix.compilers(), &(&1 == :elixir))
      
      assert position_elixir_scope < position_elixir
    end

    test "compiles project without errors" do
      assert {output, 0} = System.cmd("mix", ["compile"], cd: @test_project_path)
      assert output =~ "ElixirScope: Instrumented"
    end

    test "preserves original module behavior" do
      # Compile test module
      compile_test_module()
      
      # Test that original functionality is preserved
      assert TestModule.add(2, 3) == 5
      assert TestModule.multiply(4, 5) == 20
    end

    test "injects instrumentation calls" do
      # Compile and inspect generated bytecode
      {module, bytecode} = compile_and_get_bytecode(TestModule)
      
      # Verify instrumentation calls are present
      assert bytecode_contains_call?(bytecode, ElixirScope.Capture.InstrumentationRuntime, :report_function_entry)
      assert bytecode_contains_call?(bytecode, ElixirScope.Capture.InstrumentationRuntime, :report_function_exit)
    end
  end

  describe "AST transformation accuracy" do
    test "handles simple function definitions" do
      original_ast = quote do
        def simple_function(x) do
          x + 1
        end
      end

      transformed_ast = MixTask.transform_ast(original_ast, basic_plan())
      
      # Verify structure: entry call, original body, exit call
      assert match_instrumented_function_pattern?(transformed_ast)
    end

    test "handles functions with guards" do
      original_ast = quote do
        def guarded_function(x) when is_integer(x) do
          x * 2
        end
      end

      transformed_ast = MixTask.transform_ast(original_ast, basic_plan())
      
      # Verify guards are preserved
      assert guards_preserved?(transformed_ast)
      assert instrumentation_calls_present?(transformed_ast)
    end

    test "handles multiple function clauses" do
      original_ast = quote do
        def pattern_match(0), do: :zero
        def pattern_match(n) when n > 0, do: :positive
        def pattern_match(_), do: :negative
      end

      transformed_ast = MixTask.transform_ast(original_ast, basic_plan())
      
      # Each clause should be instrumented separately
      assert clause_count(transformed_ast) == 3
      assert all_clauses_instrumented?(transformed_ast)
    end

    test "handles GenServer callbacks" do
      original_ast = quote do
        def handle_call({:get, key}, _from, state) do
          value = Map.get(state, key)
          {:reply, value, state}
        end
      end

      plan = genserver_instrumentation_plan()
      transformed_ast = MixTask.transform_ast(original_ast, plan)
      
      # Should capture state before and after
      assert state_capture_calls_present?(transformed_ast)
    end

    test "preserves macro-generated code" do
      # Test with real Phoenix controller action
      original_ast = quote do
        def index(conn, _params) do
          users = Repo.all(User)
          render(conn, "index.html", users: users)
        end
      end

      transformed_ast = MixTask.transform_ast(original_ast, phoenix_plan())
      
      # Verify Phoenix-specific behavior is preserved
      assert phoenix_patterns_preserved?(transformed_ast)
    end
  end
end
```

#### Implementation Specification: `lib/elixir_scope/compiler/mix_task.ex`

```elixir
defmodule ElixirScope.Compiler.MixTask do
  @moduledoc """
  Mix compiler that transforms Elixir ASTs to inject ElixirScope instrumentation.
  
  This compiler:
  1. Runs before the standard Elixir compiler
  2. Transforms ASTs based on AI-generated instrumentation plans
  3. Preserves original code semantics and metadata
  4. Injects calls to ElixirScope.Capture.InstrumentationRuntime
  """
  
  use Mix.Task.Compiler
  
  alias ElixirScope.AST.Transformer
  alias ElixirScope.AI.Orchestrator
  
  @impl true
  def run(argv) do
    config = parse_argv(argv)
    
    # Get instrumentation plan from AI
    case Orchestrator.get_instrumentation_plan() do
      {:ok, plan} ->
        transform_project(plan, config)
        
      {:error, :no_plan} ->
        # Generate plan if none exists
        case Orchestrator.analyze_and_plan(File.cwd!()) do
          {:ok, plan} -> transform_project(plan, config)
          {:error, reason} -> {:error, reason}
        end
        
      {:error, reason} ->
        Mix.shell().error("ElixirScope instrumentation failed: #{inspect(reason)}")
        {:error, reason}
    end
  end
  
  defp transform_project(plan, config) do
    # Find all .ex files in the project
    elixir_files = find_elixir_files(config.source_paths)
    
    # Transform each file
    results = Enum.map(elixir_files, fn file_path ->
      transform_file(file_path, plan, config)
    end)
    
    case Enum.find(results, &match?({:error, _}, &1)) do
      nil -> 
        Mix.shell().info("ElixirScope: Instrumented #{length(elixir_files)} files")
        :ok
      {:error, reason} -> 
        {:error, reason}
    end
  end
  
  defp transform_file(file_path, plan, config) do
    try do
      # Read and parse the file
      source = File.read!(file_path)
      {:ok, ast} = Code.string_to_quoted(source, file: file_path)
      
      # Get module-specific instrumentation plan
      module_plan = extract_module_plan(ast, plan)
      
      # Transform the AST
      transformed_ast = Transformer.transform_module(ast, module_plan)
      
      # Generate output path
      output_path = generate_output_path(file_path, config)
      
      # Write transformed code
      transformed_code = Macro.to_string(transformed_ast)
      File.write!(output_path, transformed_code)
      
      :ok
    rescue
      error ->
        Mix.shell().error("Failed to transform #{file_path}: #{inspect(error)}")
        {:error, error}
    end
  end
  
  # Implementation details for file handling, path management, etc.
  defp find_elixir_files(source_paths) do
    source_paths
    |> Enum.flat_map(fn path ->
      Path.wildcard(Path.join(path, "**/*.ex"))
    end)
    |> Enum.reject(&String.contains?(&1, "/_build/"))
  end
  
  defp extract_module_plan(ast, global_plan) do
    module_name = extract_module_name(ast)
    Map.get(global_plan.modules, module_name, %{})
  end
  
  defp generate_output_path(input_path, config) do
    # Generate path in _build directory to avoid overwriting source
    relative_path = Path.relative_to(input_path, File.cwd!())
    Path.join([config.build_path, "elixir_scope", relative_path])
  end
end
```

### 1.2 AST Transformer Core

#### Test Specification: `test/elixir_scope/ast/transformer_test.exs`

```elixir
defmodule ElixirScope.AST.TransformerTest do
  use ExUnit.Case
  alias ElixirScope.AST.Transformer
  
  describe "function transformation" do
    test "transforms simple function with entry/exit instrumentation" do
      input_ast = quote do
        def calculate(x, y) do
          result = x + y
          result * 2
        end
      end
      
      plan = %{
        functions: %{
          {TestModule, :calculate, 2} => %{
            type: :full_instrumentation,
            capture_args: true,
            capture_return: true
          }
        }
      }
      
      result = Transformer.transform_function(input_ast, plan)
      
      # Verify the transformed structure
      assert {:def, _, [{:calculate, _, args}, transformed_body]} = result
      
      # Verify instrumentation calls are injected
      assert instrumentation_entry_present?(transformed_body)
      assert original_logic_preserved?(transformed_body)
      assert instrumentation_exit_present?(transformed_body)
      
      # Verify arguments are captured
      assert args_captured_in_entry?(transformed_body, length(args))
    end
    
    test "handles exception scenarios correctly" do
      input_ast = quote do
        def risky_function(x) do
          if x > 0 do
            x / (x - 1)
          else
            raise "Invalid input"
          end
        end
      end
      
      plan = %{functions: %{{TestModule, :risky_function, 1} => %{type: :full_instrumentation}}}
      result = Transformer.transform_function(input_ast, plan)
      
      # Verify try/catch wrapper is added
      assert try_catch_wrapper_present?(result)
      assert exception_reporting_present?(result)
    end
    
    test "preserves function metadata and documentation" do
      input_ast = quote do
        @doc "Calculates fibonacci number"
        @spec fibonacci(integer()) :: integer()
        def fibonacci(0), do: 0
        def fibonacci(1), do: 1
        def fibonacci(n), do: fibonacci(n-1) + fibonacci(n-2)
      end
      
      plan = %{functions: %{{TestModule, :fibonacci, 1} => %{type: :performance_only}}}
      result = Transformer.transform_function(input_ast, plan)
      
      # Verify metadata is preserved
      assert doc_attribute_preserved?(result)
      assert spec_attribute_preserved?(result)
      assert all_clauses_transformed?(result)
    end
  end
  
  describe "GenServer callback transformation" do
    test "instruments handle_call with state capture" do
      input_ast = quote do
        def handle_call({:get, key}, _from, state) do
          value = Map.get(state, key)
          {:reply, value, state}
        end
      end
      
      plan = %{
        genserver_callbacks: %{
          handle_call: %{
            capture_state_before: true,
            capture_state_after: true,
            capture_message: true
          }
        }
      }
      
      result = Transformer.transform_genserver_callback(input_ast, plan)
      
      # Verify state capture calls
      assert state_capture_before_present?(result)
      assert state_capture_after_present?(result)
      assert message_capture_present?(result)
      
      # Verify original GenServer semantics preserved
      assert genserver_return_format_preserved?(result)
    end
    
    test "handles handle_cast correctly" do
      input_ast = quote do
        def handle_cast({:update, key, value}, state) do
          new_state = Map.put(state, key, value)
          {:noreply, new_state}
        end
      end
      
      plan = %{genserver_callbacks: %{handle_cast: %{capture_state_diff: true}}}
      result = Transformer.transform_genserver_callback(input_ast, plan)
      
      # Verify state diff calculation
      assert state_diff_calculation_present?(result)
    end
  end
  
  describe "Phoenix-specific transformations" do
    test "instruments Phoenix controller actions" do
      input_ast = quote do
        def show(conn, %{"id" => id}) do
          user = Repo.get!(User, id)
          render(conn, "show.html", user: user)
        end
      end
      
      plan = %{
        phoenix_controllers: %{
          capture_params: true,
          capture_conn_state: true,
          capture_response: true
        }
      }
      
      result = Transformer.transform_phoenix_action(input_ast, plan)
      
      # Verify Phoenix-specific instrumentation
      assert params_capture_present?(result)
      assert conn_state_capture_present?(result)
      assert response_capture_present?(result)
      
      # Verify Plug behavior preserved
      assert plug_behavior_preserved?(result)
    end
    
    test "instruments LiveView callbacks" do
      input_ast = quote do
        def handle_event("increment", _params, socket) do
          new_count = socket.assigns.count + 1
          {:noreply, assign(socket, count: new_count)}
        end
      end
      
      plan = %{
        liveview_callbacks: %{
          capture_socket_assigns: true,
          capture_events: true
        }
      }
      
      result = Transformer.transform_liveview_callback(input_ast, plan)
      
      # Verify LiveView-specific instrumentation
      assert socket_assigns_capture_present?(result)
      assert event_capture_present?(result)
      assert liveview_return_format_preserved?(result)
    end
  end

  # Helper functions for verification
  defp instrumentation_entry_present?(ast) do
    # Check for ElixirScope.Capture.InstrumentationRuntime.report_function_entry call
    Macro.prewalk(ast, false, fn
      {{:., _, [_, :report_function_entry]}, _, _}, _acc -> {true, true}
      node, acc -> {node, acc}
    end) |> elem(1)
  end
  
  defp original_logic_preserved?(transformed_ast) do
    # Verify the original function body is still present and executable
    # This should check that the core logic nodes are present in the AST
    original_nodes = extract_original_logic_nodes(transformed_ast)
    length(original_nodes) > 0
  end
  
  defp instrumentation_exit_present?(ast) do
    # Similar to entry check but for exit
    Macro.prewalk(ast, false, fn
      {{:., _, [_, :report_function_exit]}, _, _}, _acc -> {true, true}
      node, acc -> {node, acc}
    end) |> elem(1)
  end
end
```

#### Implementation Specification: `lib/elixir_scope/ast/transformer.ex`

```elixir
defmodule ElixirScope.AST.Transformer do
  @moduledoc """
  Core AST transformation engine for ElixirScope instrumentation.
  
  This module provides the core logic for transforming Elixir ASTs to inject
  instrumentation calls while preserving original semantics and behavior.
  """
  
  alias ElixirScope.AST.InjectorHelpers
  
  @doc """
  Transforms a complete module AST based on the instrumentation plan.
  """
  def transform_module(ast, plan) do
    Macro.prewalk(ast, fn node ->
      case node do
        {:defmodule, meta, [module_name, [do: module_body]]} ->
          transformed_body = transform_module_body(module_body, plan)
          {:defmodule, meta, [module_name, [do: transformed_body]]}
          
        other -> other
      end
    end)
  end
  
  @doc """
  Transforms a function definition based on instrumentation plan.
  """
  def transform_function({:def, meta, [signature, body]}, plan) do
    function_name = extract_function_name(signature)
    arity = extract_arity(signature)
    
    case get_function_plan(plan, function_name, arity) do
      nil -> 
        {:def, meta, [signature, body]}
        
      function_plan ->
        transformed_body = instrument_function_body(signature, body, function_plan)
        {:def, meta, [signature, transformed_body]}
    end
  end
  
  @doc """
  Transforms GenServer callbacks with state capture.
  """
  def transform_genserver_callback({:def, meta, [signature, body]}, plan) do
    callback_name = extract_function_name(signature)
    
    case get_genserver_plan(plan, callback_name) do
      nil ->
        {:def, meta, [signature, body]}
        
      callback_plan ->
        transformed_body = instrument_genserver_body(signature, body, callback_plan)
        {:def, meta, [signature, transformed_body]}
    end
  end
  
  @doc """
  Transforms Phoenix controller actions.
  """
  def transform_phoenix_action({:def, meta, [signature, body]}, plan) do
    action_name = extract_function_name(signature)
    
    case get_phoenix_plan(plan, action_name) do
      nil ->
        {:def, meta, [signature, body]}
        
      action_plan ->
        transformed_body = instrument_phoenix_body(signature, body, action_plan)
        {:def, meta, [signature, transformed_body]}
    end
  end
  
  # Private implementation functions
  
  defp transform_module_body(body, plan) do
    Macro.prewalk(body, fn node ->
      case node do
        {:def, _, _} = function_def ->
          cond do
            genserver_callback?(function_def) ->
              transform_genserver_callback(function_def, plan)
              
            phoenix_action?(function_def) ->
              transform_phoenix_action(function_def, plan)
              
            true ->
              transform_function(function_def, plan)
          end
          
        other -> other
      end
    end)
  end
  
  defp instrument_function_body(signature, body, plan) do
    function_name = extract_function_name(signature)
    args = extract_args(signature)
    
    # Generate correlation ID
    correlation_id_var = {:correlation_id, [], __MODULE__}
    
    # Create instrumentation calls
    entry_call = InjectorHelpers.function_entry_call(
      __MODULE__, 
      function_name, 
      args, 
      correlation_id_var
    )
    
    exit_call = InjectorHelpers.function_exit_call(
      correlation_id_var,
      {:result, [], __MODULE__}
    )
    
    # Wrap body with exception handling if needed
    instrumented_body = if plan.capture_exceptions do
      wrap_with_exception_handling(body, correlation_id_var)
    else
      body
    end
    
    # Combine everything
    quote do
      unquote(correlation_id_var) = unquote(entry_call)
      
      result = try do
        unquote(instrumented_body)
      catch
        kind, reason ->
          ElixirScope.Capture.InstrumentationRuntime.report_error(
            unquote(correlation_id_var),
            kind,
            reason,
            __STACKTRACE__
          )
          :erlang.raise(kind, reason, __STACKTRACE__)
      end
      
      unquote(exit_call)
      result
    end
  end
  
  defp instrument_genserver_body(signature, body, plan) do
    callback_name = extract_function_name(signature)
    args = extract_args(signature)
    
    # GenServer callbacks have state as last argument
    state_var = List.last(args)
    
    state_capture_before = if plan.capture_state_before do
      quote do
        ElixirScope.Capture.InstrumentationRuntime.report_state_change(
          self(),
          unquote(callback_name),
          :before,
          unquote(state_var)
        )
      end
    end
    
    # Transform body and capture state after
    quote do
      unquote(state_capture_before)
      
      result = unquote(body)
      
      # Extract new state from GenServer return format
      new_state = case result do
        {:reply, _, new_state} -> new_state
        {:noreply, new_state} -> new_state
        {:stop, _, new_state} -> new_state
        {:stop, _, _, new_state} -> new_state
        other -> unquote(state_var)  # State unchanged
      end
      
      ElixirScope.Capture.InstrumentationRuntime.report_state_change(
        self(),
        unquote(callback_name),
        :after,
        new_state
      )
      
      result
    end
  end
  
  defp instrument_phoenix_body(signature, body, plan) do
    args = extract_args(signature)
    conn_var = hd(args)  # First arg is always conn
    params_var = Enum.at(args, 1, quote(do: %{}))  # Second arg is params
    
    request_capture = if plan.capture_params do
      quote do
        ElixirScope.Capture.InstrumentationRuntime.report_phoenix_request(
          unquote(conn_var),
          unquote(params_var)
        )
      end
    end
    
    quote do
      unquote(request_capture)
      
      result = unquote(body)
      
      ElixirScope.Capture.InstrumentationRuntime.report_phoenix_response(
        unquote(conn_var),
        result
      )
      
      result
    end
  end
  
  # Utility functions for AST analysis
  
  defp extract_function_name({name, _, _}), do: name
  defp extract_function_name({:when, _, [{name, _, _}, _]}), do: name
  
  defp extract_arity({_, _, args}) when is_list(args), do: length(args)
  defp extract_arity({:when, _, [{_, _, args}, _]}) when is_list(args), do: length(args)
  
  defp extract_args({_, _, args}) when is_list(args), do: args
  defp extract_args({:when, _, [{_, _, args}, _]}) when is_list(args), do: args
  
  defp genserver_callback?({:def, _, [{name, _, _}, _]}) do
    name in [:init, :handle_call, :handle_cast, :handle_info, :terminate, :code_change]
  end
  
  defp phoenix_action?({:def, _, [{name, _, args}, _]}) when is_list(args) do
    # Phoenix actions typically have (conn, params) signature
    length(args) == 2
  end
  
  defp get_function_plan(plan, name, arity) do
    Map.get(plan.functions || %{}, {__MODULE__, name, arity})
  end
  
  defp get_genserver_plan(plan, callback_name) do
    Map.get(plan.genserver_callbacks || %{}, callback_name)
  end
  
  defp get_phoenix_plan(plan, action_name) do
    Map.get(plan.phoenix_controllers || %{}, action_name)
  end
end
```

---

## 2. Phoenix Integration Layer

### Overview
The Phoenix Integration Layer provides specialized instrumentation for Phoenix applications, including controllers, LiveViews, channels, and Ecto operations. This layer must understand Phoenix's request lifecycle and provide meaningful trace correlation across the web stack.

### 2.1 Phoenix Request Lifecycle Tracing

#### Test Specification: `test/elixir_scope/phoenix/integration_test.exs`

```elixir
defmodule ElixirScope.Phoenix.IntegrationTest do
  use ExUnit.Case
  use Phoenix.ConnTest
  
  alias ElixirScope.Phoenix.Integration
  alias ElixirScope.Storage.DataAccess
  
  @endpoint TestPhoenixApp.Endpoint
  
  setup do
    # Start test Phoenix app with ElixirScope
    {:ok, _} = TestPhoenixApp.start_link()
    
    # Clear any existing traces
    DataAccess.clear_all()
    
    # Enable Phoenix instrumentation
    Integration.enable()
    
    :ok
  end
  
  describe "HTTP request tracing" do
    test "traces complete GET request lifecycle" do
      # Make request to instrumented Phoenix app
      conn = get(build_conn(), "/users/123")
      
      # Wait for async processing
      Process.sleep(100)
      
      # Verify trace events were captured
      events = DataAccess.get_events_by_correlation(extract_correlation_id(conn))
      
      # Should have request entry, controller action, view render, response
      assert_event_sequence(events, [
        :phoenix_request_start,
        :phoenix_controller_entry,
        :phoenix_controller_exit,
        :phoenix_view_render,
        :phoenix_request_complete
      ])
      
      # Verify request details are captured
      request_event = find_event(events, :phoenix_request_start)
      assert request_event.data.method == "GET"
      assert request_event.data.path == "/users/123"
      assert request_event.data.params == %{"id" => "123"}
    end
    
    test "traces POST request with parameters" do
      params = %{"user" => %{"name" => "John", "email" => "john@example.com"}}
      conn = post(build_conn(), "/users", params)
      
      Process.sleep(100)
      
      events = DataAccess.get_events_by_correlation(extract_correlation_id(conn))
      
      # Verify parameters are captured
      request_event = find_event(events, :phoenix_request_start)
      assert request_event.data.params == params
      
      # Verify database operations are correlated
      db_events = filter_events(events, :ecto_query)
      assert length(db_events) > 0
      
      # All events should have same correlation ID
      correlation_ids = Enum.map(events, & &1.correlation_id) |> Enum.uniq()
      assert length(correlation_ids) == 1
    end
    
    test "traces error scenarios" do
      # Request that will cause 500 error
      conn = get(build_conn(), "/users/nonexistent")
      
      Process.sleep(100)
      
      events = DataAccess.get_events_by_correlation(extract_correlation_id(conn))
      
      # Should have error event
      error_event = find_event(events, :phoenix_error)
      assert error_event.data.status == 500
      assert error_event.data.error_type == :not_found
    end
  end
  
  describe "LiveView tracing" do
    test "traces LiveView mount and lifecycle" do
      # Connect to LiveView
      {:ok, view, _html} = live(build_conn(), "/live/counter")
      
      Process.sleep(50)
      
      # Get mount events
      mount_events = DataAccess.get_events_by_type(:liveview_mount)
      assert length(mount_events) == 1
      
      mount_event = hd(mount_events)
      assert mount_event.data.assigns.count == 0
    end
    
    test "traces LiveView events and state changes" do
      {:ok, view, _html} = live(build_conn(), "/live/counter")
      
      # Trigger event
      view |> element("button", "Increment") |> render_click()
      
      Process.sleep(50)
      
      # Get handle_event trace
      event_traces = DataAccess.get_events_by_type(:liveview_handle_event)
      assert length(event_traces) == 1
      
      event_trace = hd(event_traces)
      assert event_trace.data.event == "increment"
      assert event_trace.data.old_assigns.count == 0
      assert event_trace.data.new_assigns.count == 1
    end
    
    test "correlates LiveView events with backend operations" do
      {:ok, view, _html} = live(build_conn(), "/live/users")
      
      # Trigger event that loads data
      view |> element("button", "Load Users") |> render_click()
      
      Process.sleep(100)
      
      # Get correlation ID from LiveView event
      lv_event = DataAccess.get_events_by_type(:liveview_handle_event) |> hd()
      correlation_id = lv_event.correlation_id
      
      # Verify database query is correlated
      all_events = DataAccess.get_events_by_correlation(correlation_id)
      db_events = filter_events(all_events, :ecto_query)
      
      assert length(db_events) > 0
      assert Enum.all?(db_events, &(&1.correlation_id == correlation_id))
    end
  end
  
  describe "Phoenix Channel tracing" do
    test "traces channel join and message flow" do
      # Connect to channel
      {:ok, socket} = connect(TestSocket, %{})
      {:ok, _, socket} = subscribe_and_join(socket, "room:lobby", %{})
      
      Process.sleep(50)
      
      # Verify join events
      join_events = DataAccess.get_events_by_type(:phoenix_channel_join)
      assert length(join_events) == 1
      
      join_event = hd(join_events)
      assert join_event.data.topic == "room:lobby"
    end
    
    test "traces channel message handling" do
      {:ok, socket} = connect(TestSocket, %{})
      {:ok, _, socket} = subscribe_and_join(socket, "room:lobby", %{})
      
      # Send message
      push(socket, "new_message", %{"text" => "Hello World"})
      
      Process.sleep(50)
      
      # Verify message events
      message_events = DataAccess.get_events_by_type(:phoenix_channel_message)
      assert length(message_events) == 1
      
      message_event = hd(message_events)
      assert message_event.data.event == "new_message"
      assert message_event.data.payload.text == "Hello World"
    end
  end
end
```

#### Implementation Specification: `lib/elixir_scope/phoenix/integration.ex`

```elixir
defmodule ElixirScope.Phoenix.Integration do
  @moduledoc """
  Phoenix-specific integration for ElixirScope instrumentation.
  
  This module provides specialized tracing for Phoenix applications including:
  - HTTP request/response lifecycle
  - LiveView mount, events, and state changes
  - Channel connections and message flow
  - Ecto query correlation
  """
  
  alias ElixirScope.Capture.InstrumentationRuntime
  alias ElixirScope.Utils
  
  @doc """
  Enables Phoenix instrumentation by attaching telemetry handlers.
  """
  def enable do
    attach_http_handlers()
    attach_liveview_handlers()
    attach_channel_handlers()
    attach_ecto_handlers()
  end
  
  @doc """
  Disables Phoenix instrumentation.
  """
  def disable do
    :telemetry.detach_many([
      :elixir_scope_phoenix_http,
      :elixir_scope_phoenix_liveview,
      :elixir_scope_phoenix_channel,
      :elixir_scope_phoenix_ecto
    ])
  end
  
  # HTTP Request/Response Handlers
  
  defp attach_http_handlers do
    :telemetry.attach_many(
      :elixir_scope_phoenix_http,
      [
        [:phoenix, :endpoint, :start],
        [:phoenix, :endpoint, :stop],
        [:phoenix, :router_dispatch, :start],
        [:phoenix, :router_dispatch, :stop],
        [:phoenix, :controller, :start],
        [:phoenix, :controller, :stop]
      ],
      &handle_http_event/4,
      %{}
    )
  end
  
  def handle_http_event([:phoenix, :endpoint, :start], measurements, metadata, _config) do
    correlation_id = generate_correlation_id()
    
    # Store correlation ID in conn for downstream use
    conn = put_correlation_id(metadata.conn, correlation_id)
    
    InstrumentationRuntime.report_phoenix_request_start(
      correlation_id,
      conn.method,
      conn.request_path,
      conn.params,
      conn.remote_ip
    )
    
    # Update metadata for downstream handlers
    %{metadata | conn: conn}
  end
  
  def handle_http_event([:phoenix, :endpoint, :stop], measurements, metadata, _config) do
    correlation_id = get_correlation_id(metadata.conn)
    
    InstrumentationRuntime.report_phoenix_request_complete(
      correlation_id,
      metadata.conn.status,
      measurements.duration,
      response_size(metadata.conn)
    )
  end
  
  def handle_http_event([:phoenix, :controller, :start], measurements, metadata, _config) do
    correlation_id = get_correlation_id(metadata.conn)
    
    InstrumentationRuntime.report_phoenix_controller_entry(
      correlation_id,
      metadata.controller,
      metadata.action,
      metadata.params
    )
  end
  
  def handle_http_event([:phoenix, :controller, :stop], measurements, metadata, _config) do
    correlation_id = get_correlation_id(metadata.conn)
    
    InstrumentationRuntime.report_phoenix_controller_exit(
      correlation_id,
      metadata.controller,
      metadata.action,
      measurements.duration
    )
  end
  
  # LiveView Handlers
  
  defp attach_liveview_handlers do
    :telemetry.attach_many(
      :elixir_scope_phoenix_liveview,
      [
        [:phoenix, :live_view, :mount, :start],
        [:phoenix, :live_view, :mount, :stop],
        [:phoenix, :live_view, :handle_event, :start],
        [:phoenix, :live_view, :handle_event, :stop],
        [:phoenix, :live_view, :handle_info, :start],
        [:phoenix, :live_view, :handle_info, :stop]
      ],
      &handle_liveview_event/4,
      %{}
    )
  end
  
  def handle_liveview_event([:phoenix, :live_view, :mount, :start], _measurements, metadata, _config) do
    correlation_id = generate_correlation_id()
    
    # Store correlation ID in socket for downstream use
    socket = put_socket_correlation_id(metadata.socket, correlation_id)
    
    InstrumentationRuntime.report_liveview_mount_start(
      correlation_id,
      metadata.module,
      metadata.params,
      socket.assigns
    )
    
    %{metadata | socket: socket}
  end
  
  def handle_liveview_event([:phoenix, :live_view, :mount, :stop], measurements, metadata, _config) do
    correlation_id = get_socket_correlation_id(metadata.socket)
    
    InstrumentationRuntime.report_liveview_mount_complete(
      correlation_id,
      metadata.socket.assigns,
      measurements.duration
    )
  end
  
  def handle_liveview_event([:phoenix, :live_view, :handle_event, :start], _measurements, metadata, _config) do
    correlation_id = get_socket_correlation_id(metadata.socket)
    
    InstrumentationRuntime.report_liveview_handle_event_start(
      correlation_id,
      metadata.event,
      metadata.params,
      metadata.socket.assigns
    )
  end
  
  def handle_liveview_event([:phoenix, :live_view, :handle_event, :stop], measurements, metadata, _config) do
    correlation_id = get_socket_correlation_id(metadata.socket)
    
    # Capture state changes
    old_assigns = get_previous_assigns(metadata.socket)
    new_assigns = metadata.socket.assigns
    
    InstrumentationRuntime.report_liveview_handle_event_complete(
      correlation_id,
      metadata.event,
      old_assigns,
      new_assigns,
      measurements.duration
    )
  end
  
  # Channel Handlers
  
  defp attach_channel_handlers do
    :telemetry.attach_many(
      :elixir_scope_phoenix_channel,
      [
        [:phoenix, :channel, :join, :start],
        [:phoenix, :channel, :join, :stop],
        [:phoenix, :channel, :handle_in, :start],
        [:phoenix, :channel, :handle_in, :stop]
      ],
      &handle_channel_event/4,
      %{}
    )
  end
  
  def handle_channel_event([:phoenix, :channel, :join, :start], _measurements, metadata, _config) do
    correlation_id = generate_correlation_id()
    
    InstrumentationRuntime.report_phoenix_channel_join_start(
      correlation_id,
      metadata.socket.channel,
      metadata.socket.topic,
      metadata.params
    )
  end
  
  def handle_channel_event([:phoenix, :channel, :join, :stop], measurements, metadata, _config) do
    InstrumentationRuntime.report_phoenix_channel_join_complete(
      metadata.socket.channel,
      metadata.socket.topic,
      measurements.duration,
      metadata.result
    )
  end
  
  def handle_channel_event([:phoenix, :channel, :handle_in, :start], _measurements, metadata, _config) do
    correlation_id = generate_correlation_id()
    
    InstrumentationRuntime.report_phoenix_channel_message_start(
      correlation_id,
      metadata.socket.channel,
      metadata.event,
      metadata.payload
    )
  end
  
  def handle_channel_event([:phoenix, :channel, :handle_in, :stop], measurements, metadata, _config) do
    InstrumentationRuntime.report_phoenix_channel_message_complete(
      metadata.socket.channel,
      metadata.event,
      measurements.duration,
      metadata.result
    )
  end
  
  # Ecto Query Handlers
  
  defp attach_ecto_handlers do
    :telemetry.attach_many(
      :elixir_scope_phoenix_ecto,
      [
        [:ecto, :repo, :query, :start],
        [:ecto, :repo, :query, :stop]
      ],
      &handle_ecto_event/4,
      %{}
    )
  end
  
  def handle_ecto_event([:ecto, :repo, :query, :start], _measurements, metadata, _config) do
    # Try to get correlation ID from current process
    correlation_id = get_process_correlation_id() || generate_correlation_id()
    
    InstrumentationRuntime.report_ecto_query_start(
      correlation_id,
      metadata.repo,
      metadata.source,
      sanitize_query(metadata.query),
      length(metadata.params || [])
    )
  end
  
  def handle_ecto_event([:ecto, :repo, :query, :stop], measurements, metadata, _config) do
    correlation_id = get_process_correlation_id()
    
    InstrumentationRuntime.report_ecto_query_complete(
      correlation_id,
      metadata.repo,
      measurements.query_time,
      measurements.decode_time,
      metadata.result
    )
  end
  
  # Utility Functions
  
  defp generate_correlation_id do
    Utils.generate_correlation_id()
  end
  
  defp put_correlation_id(conn, correlation_id) do
    Plug.Conn.put_private(conn, :elixir_scope_correlation_id, correlation_id)
  end
  
  defp get_correlation_id(conn) do
    conn.private[:elixir_scope_correlation_id]
  end
  
  defp put_socket_correlation_id(socket, correlation_id) do
    Phoenix.LiveView.assign(socket, :elixir_scope_correlation_id, correlation_id)
  end
  
  defp get_socket_correlation_id(socket) do
    socket.assigns[:elixir_scope_correlation_id]
  end
  
  defp get_process_correlation_id do
    Process.get(:elixir_scope_correlation_id)
  end
  
  defp put_process_correlation_id(correlation_id) do
    Process.put(:elixir_scope_correlation_id, correlation_id)
  end
  
  defp response_size(conn) do
    case Plug.Conn.get_resp_header(conn, "content-length") do
      [size] -> String.to_integer(size)
      _ -> byte_size(conn.resp_body || "")
    end
  end
  
  defp get_previous_assigns(socket) do
    # This would need to be stored during previous operations
    Process.get({:elixir_scope_previous_assigns, socket.id}, %{})
  end
  
  defp sanitize_query(query) do
    # Remove sensitive data from query for logging
    String.replace(query, ~r/\$\d+/, "?")
  end
end
```

### 2.2 Phoenix Test Application

#### Test Specification: `test/support/test_phoenix_app.ex`

```elixir
defmodule TestPhoenixApp do
  @moduledoc """
  Test Phoenix application for validating ElixirScope integration.
  """
  
  use Application
  
  def start(_type, _args) do
    children = [
      TestPhoenixApp.Repo,
      TestPhoenixApp.Endpoint,
      {Phoenix.PubSub, name: TestPhoenixApp.PubSub}
    ]
    
    opts = [strategy: :one_for_one, name: TestPhoenixApp.Supervisor]
    Supervisor.start_link(children, opts)
  end
end

defmodule TestPhoenixApp.Router do
  use Phoenix.Router
  import Phoenix.Controller
  import Phoenix.LiveView.Router
  
  pipeline :browser do
    plug :accepts, ["html"]
  end
  
  scope "/", TestPhoenixApp do
    pipe_through :browser
    
    get "/users/:id", UserController, :show
    post "/users", UserController, :create
    get "/users/nonexistent", UserController, :error_test
    
    live "/live/counter", CounterLive
    live "/live/users", UsersLive
  end
end

defmodule TestPhoenixApp.UserController do
  use Phoenix.Controller
  
  def show(conn, %{"id" => id}) do
    # Simulate database lookup that will be traced
    user = TestPhoenixApp.Repo.get!(User, id)
    render(conn, "show.html", user: user)
  end
  
  def create(conn, %{"user" => user_params}) do
    # Simulate user creation with validation
    case TestPhoenixApp.Repo.insert(%User{}, user_params) do
      {:ok, user} ->
        render(conn, "show.html", user: user)
      {:error, changeset} ->
        render(conn, "new.html", changeset: changeset)
    end
  end
  
  def error_test(conn, _params) do
    # Intentionally cause error for testing
    raise "Test error for ElixirScope tracing"
  end
end

defmodule TestPhoenixApp.CounterLive do
  use Phoenix.LiveView
  
  def mount(_params, _session, socket) do
    {:ok, assign(socket, count: 0)}
  end
  
  def handle_event("increment", _params, socket) do
    new_count = socket.assigns.count + 1
    {:noreply, assign(socket, count: new_count)}
  end
  
  def render(assigns) do
    ~H"""
    <div>
      <p>Count: <%= @count %></p>
      <button phx-click="increment">Increment</button>
    </div>
    """
  end
end

defmodule TestPhoenixApp.UsersLive do
  use Phoenix.LiveView
  
  def mount(_params, _session, socket) do
    {:ok, assign(socket, users: [])}
  end
  
  def handle_event("load_users", _params, socket) do
    # This will trigger Ecto queries that should be correlated
    users = TestPhoenixApp.Repo.all(User)
    {:noreply, assign(socket, users: users)}
  end
  
  def render(assigns) do
    ~H"""
    <div>
      <button phx-click="load_users">Load Users</button>
      <%= for user <- @users do %>
        <p><%= user.name %></p>
      <% end %>
    </div>
    """
  end
end
```

---

## 3. AI Code Analysis Engine

### Overview
The AI Code Analysis Engine provides intelligent analysis of Elixir codebases to generate optimal instrumentation plans. Initially implemented with rule-based heuristics, it's designed to be enhanced with actual LLM integration.

### 3.1 Rule-Based Code Analysis

#### Test Specification: `test/elixir_scope/ai/code_analyzer_test.exs`

```elixir
defmodule ElixirScope.AI.CodeAnalyzerTest do
  use ExUnit.Case
  alias ElixirScope.AI.CodeAnalyzer
  
  @test_project_path "test/fixtures/sample_elixir_project"
  
  describe "module pattern recognition" do
    test "identifies GenServer modules correctly" do
      genserver_code = """
      defmodule MyApp.UserServer do
        use GenServer
        
        def start_link(opts) do
          GenServer.start_link(__MODULE__, opts, name: __MODULE__)
        end
        
        def init(state) do
          {:ok, state}
        end
        
        def handle_call({:get, key}, _from, state) do
          {:reply, Map.get(state, key), state}
        end
      end
      """
      
      analysis = CodeAnalyzer.analyze_code(genserver_code)
      
      assert analysis.module_type == :genserver
      assert length(analysis.callbacks) == 3
      assert :handle_call in analysis.callbacks
      assert analysis.state_complexity == :medium
      assert analysis.recommended_instrumentation == :state_tracking
    end
    
    test "identifies Phoenix controllers" do
      controller_code = """
      defmodule MyApp.UserController do
        use MyApp, :controller
        
        def index(conn, _params) do
          users = Repo.all(User)
          render(conn, "index.html", users: users)
        end
        
        def show(conn, %{"id" => id}) do
          user = Repo.get!(User, id)
          render(conn, "show.html", user: user)
        end
      end
      """
      
      analysis = CodeAnalyzer.analyze_code(controller_code)
      
      assert analysis.module_type == :phoenix_controller
      assert length(analysis.actions) == 2
      assert :index in analysis.actions
      assert :show in analysis.actions
      assert analysis.database_interactions == true
      assert analysis.recommended_instrumentation == :request_lifecycle
    end
    
    test "identifies LiveView modules" do
      liveview_code = """
      defmodule MyApp.CounterLive do
        use Phoenix.LiveView
        
        def mount(_params, _session, socket) do
          {:ok, assign(socket, count: 0)}
        end
        
        def handle_event("increment", _params, socket) do
          {:noreply, assign(socket, count: socket.assigns.count + 1)}
        end
      end
      """
      
      analysis = CodeAnalyzer.analyze_code(liveview_code)
      
      assert analysis.module_type == :phoenix_liveview
      assert analysis.has_mount == true
      assert length(analysis.events) == 1
      assert "increment" in analysis.events
      assert analysis.recommended_instrumentation == :liveview_lifecycle
    end
    
    test "identifies supervision trees" do
      supervisor_code = """
      defmodule MyApp.Supervisor do
        use Supervisor
        
        def start_link(opts) do
          Supervisor.start_link(__MODULE__, opts, name: __MODULE__)
        end
        
        def init(_opts) do
          children = [
            MyApp.UserServer,
            {MyApp.WorkerPool, pool_size: 5},
            {Phoenix.PubSub, name: MyApp.PubSub}
          ]
          
          Supervisor.init(children, strategy: :one_for_one)
        end
      end
      """
      
      analysis = CodeAnalyzer.analyze_code(supervisor_code)
      
      assert analysis.module_type == :supervisor
      assert length(analysis.children) == 3
      assert analysis.strategy == :one_for_one
      assert analysis.recommended_instrumentation == :process_lifecycle
    end
  end
  
  describe "complexity analysis" do
    test "calculates function complexity correctly" do
      complex_function = """
      def complex_calculation(data) do
        data
        |> Enum.map(fn x -> 
          if x > 0 do
            case expensive_operation(x) do
              {:ok, result} -> 
                result
                |> process_further()
                |> validate_result()
              {:error, reason} -> 
                handle_error(reason)
                retry_operation(x)
            end
          else
            default_value()
          end
        end)
        |> Enum.filter(&valid?/1)
        |> Enum.reduce(0, &+/2)
      end
      """
      
      analysis = CodeAnalyzer.analyze_function(complex_function)
      
      assert analysis.complexity_score >= 8
      assert analysis.nesting_depth >= 3
      assert analysis.recommended_instrumentation == :detailed_tracing
    end
    
    test "identifies performance-critical patterns" do
      performance_critical = """
      def process_large_dataset(dataset) do
        dataset
        |> Enum.map(&expensive_computation/1)
        |> Enum.chunk_every(1000)
        |> Enum.map(&parallel_process/1)
        |> List.flatten()
      end
      """
      
      analysis = CodeAnalyzer.analyze_function(performance_critical)
      
      assert analysis.performance_critical == true
      assert analysis.recommended_instrumentation == :performance_monitoring
    end
  end
  
  describe "project-wide analysis" do
    test "analyzes complete project structure" do
      project_analysis = CodeAnalyzer.analyze_project(@test_project_path)
      
      # Verify comprehensive analysis
      assert project_analysis.total_modules > 0
      assert project_analysis.genserver_modules > 0
      assert project_analysis.phoenix_modules > 0
      assert project_analysis.supervision_tree != nil
      
      # Verify instrumentation recommendations
      assert project_analysis.recommended_plan != nil
      assert project_analysis.estimated_overhead < 0.05  # <5%
      
      # Verify dependency analysis
      assert project_analysis.external_dependencies != nil
      assert project_analysis.internal_message_flows != nil
    end
    
    test "generates prioritized instrumentation plan" do
      plan = CodeAnalyzer.generate_instrumentation_plan(@test_project_path)
      
      # Verify plan structure
      assert plan.priority_modules != nil
      assert plan.instrumentation_strategies != nil
      assert plan.estimated_impact != nil
      
      # Verify priorities are ordered correctly
      assert hd(plan.priority_modules).priority == :high
      
      # Verify specific recommendations
      critical_module = find_module(plan.priority_modules, MyApp.CriticalGenServer)
      assert critical_module.instrumentation_type == :full_tracing
      assert critical_module.reason =~ "high complexity"
    end
  end
  
  describe "message flow analysis" do
    test "identifies inter-process communication patterns" do
      genserver_caller = """
      def get_user_data(user_id) do
        GenServer.call(UserServer, {:get, user_id})
      end
      """
      
      analysis = CodeAnalyzer.analyze_message_flows([genserver_caller])
      
      assert length(analysis.call_patterns) == 1
      call_pattern = hd(analysis.call_patterns)
      assert call_pattern.target_server == UserServer
      assert call_pattern.message_type == :call
      assert call_pattern.recommended_correlation == true
    end
    
    test "identifies PubSub patterns" do
      pubsub_code = """
      def broadcast_update(user_id, data) do
        Phoenix.PubSub.broadcast(MyApp.PubSub, "user:\#{user_id}", {:user_updated, data})
      end
      
      def handle_info({:user_updated, data}, socket) do
        {:noreply, update_user_data(socket, data)}
      end
      """
      
      analysis = CodeAnalyzer.analyze_message_flows([pubsub_code])
      
      assert length(analysis.pubsub_patterns) == 1
      pattern = hd(analysis.pubsub_patterns)
      assert pattern.topic_pattern == "user:*"
      assert pattern.message_type == :user_updated
      assert pattern.recommended_tracing == :broadcast_correlation
    end
  end
end
```

#### Implementation Specification: `lib/elixir_scope/ai/code_analyzer.ex`

```elixir
defmodule ElixirScope.AI.CodeAnalyzer do
  @moduledoc """
  AI-powered code analysis engine for ElixirScope.
  
  Analyzes Elixir codebases to understand structure, complexity, and patterns
  to generate intelligent instrumentation recommendations.
  
  Initially implemented with rule-based heuristics, designed to be enhanced
  with actual LLM integration.
  """
  
  alias ElixirScope.AI.PatternRecognizer
  alias ElixirScope.AI.ComplexityAnalyzer
  alias ElixirScope.AI.InstrumentationPlanner
  
  defstruct [
    :module_type,
    :complexity_score,
    :callbacks,
    :actions,
    :events,
    :children,
    :strategy,
    :state_complexity,
    :performance_critical,
    :database_interactions,
    :recommended_instrumentation,
    :confidence_score
  ]
  
  @doc """
  Analyzes a single piece of Elixir code and returns analysis results.
  """
  def analyze_code(code_string) when is_binary(code_string) do
    with {:ok, ast} <- Code.string_to_quoted(code_string) do
      analyze_ast(ast)
    else
      {:error, _} -> {:error, :invalid_code}
    end
  end
  
  @doc """
  Analyzes a complete Elixir project directory.
  """
  def analyze_project(project_path) do
    elixir_files = find_elixir_files(project_path)
    
    module_analyses = 
      elixir_files
      |> Enum.map(&analyze_file/1)
      |> Enum.reject(&match?({:error, _}, &1))
    
    project_structure = analyze_project_structure(module_analyses)
    supervision_tree = build_supervision_tree(module_analyses)
    message_flows = analyze_inter_module_communication(module_analyses)
    
    %{
      total_modules: length(module_analyses),
      genserver_modules: count_by_type(module_analyses, :genserver),
      phoenix_modules: count_by_type(module_analyses, :phoenix_controller) + 
                      count_by_type(module_analyses, :phoenix_liveview),
      supervision_tree: supervision_tree,
      project_structure: project_structure,
      external_dependencies: extract_dependencies(module_analyses),
      internal_message_flows: message_flows,
      recommended_plan: generate_project_plan(module_analyses),
      estimated_overhead: calculate_estimated_overhead(module_analyses)
    }
  end
  
  @doc """
  Generates an instrumentation plan for a project.
  """
  def generate_instrumentation_plan(project_path) do
    project_analysis = analyze_project(project_path)
    
    priority_modules = prioritize_modules(project_analysis)
    instrumentation_strategies = generate_strategies(priority_modules)
    
    %{
      priority_modules: priority_modules,
      instrumentation_strategies: instrumentation_strategies,
      estimated_impact: calculate_impact(instrumentation_strategies),
      configuration: generate_configuration(instrumentation_strategies)
    }
  end
  
  @doc """
  Analyzes message flow patterns across modules.
  """
  def analyze_message_flows(code_samples) do
    call_patterns = extract_call_patterns(code_samples)
    pubsub_patterns = extract_pubsub_patterns(code_samples)
    
    %{
      call_patterns: call_patterns,
      pubsub_patterns: pubsub_patterns,
      correlation_opportunities: identify_correlation_opportunities(call_patterns, pubsub_patterns)
    }
  end
  
  @doc """
  Analyzes a single function for complexity and performance characteristics.
  """
  def analyze_function(function_code) do
    {:ok, ast} = Code.string_to_quoted(function_code)
    
    complexity = ComplexityAnalyzer.calculate_complexity(ast)
    performance_indicators = analyze_performance_patterns(ast)
    
    %{
      complexity_score: complexity.score,
      nesting_depth: complexity.nesting_depth,
      performance_critical: performance_indicators.is_critical,
      recommended_instrumentation: recommend_function_instrumentation(complexity, performance_indicators)
    }
  end
  
  # Private implementation functions
  
  defp analyze_ast(ast) do
    module_type = PatternRecognizer.identify_module_type(ast)
    complexity = ComplexityAnalyzer.analyze_module(ast)
    patterns = PatternRecognizer.extract_patterns(ast)
    
    %__MODULE__{
      module_type: module_type,
      complexity_score: complexity.score,
      callbacks: patterns.callbacks,
      actions: patterns.actions,
      events: patterns.events,
      children: patterns.children,
      strategy: patterns.strategy,
      state_complexity: complexity.state_complexity,
      performance_critical: complexity.performance_critical,
      database_interactions: patterns.database_interactions,
      recommended_instrumentation: recommend_instrumentation(module_type, complexity, patterns),
      confidence_score: calculate_confidence(module_type, patterns)
    }
  end
  
  defp analyze_file(file_path) do
    try do
      code = File.read!(file_path)
      analysis = analyze_code(code)
      {:ok, Map.put(analysis, :file_path, file_path)}
    rescue
      error -> {:error, {file_path, error}}
    end
  end
  
  defp find_elixir_files(project_path) do
    Path.wildcard(Path.join([project_path, "**", "*.ex"]))
    |> Enum.reject(&String.contains?(&1, "/_build/"))
    |> Enum.reject(&String.contains?(&1, "/deps/"))
  end
  
  defp analyze_project_structure(module_analyses) do
    modules_by_type = Enum.group_by(module_analyses, & &1.module_type)
    
    %{
      genservers: Map.get(modules_by_type, :genserver, []),
      supervisors: Map.get(modules_by_type, :supervisor, []),
      phoenix_controllers: Map.get(modules_by_type, :phoenix_controller, []),
      phoenix_liveviews: Map.get(modules_by_type, :phoenix_liveview, []),
      regular_modules: Map.get(modules_by_type, :regular, [])
    }
  end
  
  defp build_supervision_tree(module_analyses) do
    supervisors = Enum.filter(module_analyses, &(&1.module_type == :supervisor))
    workers = Enum.filter(module_analyses, &(&1.module_type == :genserver))
    
    # Build tree structure based on children specifications
    Enum.map(supervisors, fn supervisor ->
      children = match_supervisor_children(supervisor, workers)
      %{
        supervisor: supervisor,
        children: children,
        strategy: supervisor.strategy
      }
    end)
  end
  
  defp analyze_inter_module_communication(module_analyses) do
    # Extract GenServer.call/cast patterns
    call_patterns = extract_genserver_calls(module_analyses)
    
    # Extract PubSub patterns
    pubsub_patterns = extract_pubsub_usage(module_analyses)
    
    %{
      genserver_calls: call_patterns,
      pubsub_usage: pubsub_patterns,
      process_links: analyze_process_links(module_analyses)
    }
  end
  
  defp recommend_instrumentation(module_type, complexity, patterns) do
    base_recommendation = case module_type do
      :genserver -> 
        if complexity.state_complexity == :high do
          :full_state_tracking
        else
          :basic_state_tracking
        end
        
      :supervisor ->
        :process_lifecycle
        
      :phoenix_controller ->
        if patterns.database_interactions do
          :request_lifecycle_with_db
        else
          :request_lifecycle
        end
        
      :phoenix_liveview ->
        :liveview_lifecycle
        
      _ ->
        if complexity.performance_critical do
          :performance_monitoring
        else
          :minimal
        end
    end
    
    # Adjust based on complexity
    if complexity.score > 10 do
      enhance_instrumentation(base_recommendation)
    else
      base_recommendation
    end
  end
  
  defp prioritize_modules(project_analysis) do
    all_modules = extract_all_modules(project_analysis)
    
    # Score modules based on multiple factors
    scored_modules = Enum.map(all_modules, fn module ->
      score = calculate_priority_score(module, project_analysis)
      priority = determine_priority_level(score)
      
      %{
        module: module,
        score: score,
        priority: priority,
        instrumentation_type: recommend_detailed_instrumentation(module, priority),
        reason: generate_priority_reason(module, score)
      }
    end)
    
    # Sort by score descending
    Enum.sort_by(scored_modules, & &1.score, :desc)
  end
  
  defp calculate_priority_score(module, project_analysis) do
    base_score = case module.module_type do
      :genserver -> 8
      :supervisor -> 6
      :phoenix_controller -> 7
      :phoenix_liveview -> 7
      _ -> 3
    end
    
    complexity_bonus = min(module.complexity_score, 5)
    critical_path_bonus = if in_critical_path?(module, project_analysis), do: 3, else: 0
    performance_bonus = if module.performance_critical, do: 4, else: 0
    
    base_score + complexity_bonus + critical_path_bonus + performance_bonus
  end
  
  defp determine_priority_level(score) do
    cond do
      score >= 15 -> :critical
      score >= 10 -> :high
      score >= 6 -> :medium
      true -> :low
    end
  end
  
  defp recommend_detailed_instrumentation(module, priority) do
    base_type = module.recommended_instrumentation
    
    case priority do
      :critical -> enhance_to_full_tracing(base_type)
      :high -> enhance_to_detailed_tracing(base_type)
      :medium -> base_type
      :low -> simplify_instrumentation(base_type)
    end
  end
  
  defp generate_strategies(priority_modules) do
    Enum.reduce(priority_modules, %{}, fn module_info, acc ->
      strategy = create_instrumentation_strategy(module_info)
      Map.put(acc, module_info.module.file_path, strategy)
    end)
  end
  
  defp create_instrumentation_strategy(module_info) do
    %{
      module_type: module_info.module.module_type,
      instrumentation_level: module_info.instrumentation_type,
      specific_functions: select_functions_to_instrument(module_info),
      capture_settings: generate_capture_settings(module_info),
      sampling_rate: calculate_sampling_rate(module_info.priority),
      performance_budget: calculate_performance_budget(module_info.priority)
    }
  end
  
  defp select_functions_to_instrument(module_info) do
    case module_info.module.module_type do
      :genserver ->
        base_callbacks = [:init, :handle_call, :handle_cast, :handle_info]
        if module_info.priority in [:critical, :high] do
          base_callbacks ++ [:terminate, :code_change]
        else
          base_callbacks
        end
        
      :phoenix_controller ->
        # Instrument all public actions
        module_info.module.actions || []
        
      :phoenix_liveview ->
        base_callbacks = [:mount, :handle_event]
        if module_info.priority == :critical do
          base_callbacks ++ [:handle_info, :handle_params]
        else
          base_callbacks
        end
        
      _ ->
        # For regular modules, instrument high-complexity functions
        []  # Would need to extract function list from AST
    end
  end
  
  defp generate_capture_settings(module_info) do
    %{
      capture_args: module_info.priority in [:critical, :high],
      capture_return: module_info.priority in [:critical, :high],
      capture_state: module_info.module.module_type in [:genserver, :phoenix_liveview],
      capture_exceptions: true,
      capture_performance: module_info.module.performance_critical
    }
  end
  
  defp extract_call_patterns(code_samples) do
    Enum.flat_map(code_samples, fn code ->
      {:ok, ast} = Code.string_to_quoted(code)
      
      Macro.prewalk(ast, [], fn
        # GenServer.call patterns
        {{:., _, [{:__aliases__, _, [:GenServer]}, :call]}, _, [target, message]} = node, acc ->
          pattern = %{
            type: :genserver_call,
            target_server: extract_target(target),
            message_type: :call,
            message_pattern: extract_message_pattern(message),
            recommended_correlation: true
          }
          {node, [pattern | acc]}
          
        # GenServer.cast patterns  
        {{:., _, [{:__aliases__, _, [:GenServer]}, :cast]}, _, [target, message]} = node, acc ->
          pattern = %{
            type: :genserver_cast,
            target_server: extract_target(target),
            message_type: :cast,
            message_pattern: extract_message_pattern(message),
            recommended_correlation: true
          }
          {node, [pattern | acc]}
          
        node, acc -> {node, acc}
      end) |> elem(1)
    end)
  end
  
  defp extract_pubsub_patterns(code_samples) do
    Enum.flat_map(code_samples, fn code ->
      {:ok, ast} = Code.string_to_quoted(code)
      
      Macro.prewalk(ast, [], fn
        # Phoenix.PubSub.broadcast patterns
        {{:., _, [{:__aliases__, _, [:Phoenix, :PubSub]}, :broadcast]}, _, [pubsub, topic, message]} = node, acc ->
          pattern = %{
            type: :pubsub_broadcast,
            pubsub_server: extract_pubsub_server(pubsub),
            topic_pattern: extract_topic_pattern(topic),
            message_type: extract_message_type(message),
            recommended_tracing: :broadcast_correlation
          }
          {node, [pattern | acc]}
          
        node, acc -> {node, acc}
      end) |> elem(1)
    end)
  end
  
  # Utility functions
  
  defp count_by_type(analyses, type) do
    Enum.count(analyses, &(&1.module_type == type))
  end
  
  defp extract_dependencies(module_analyses) do
    Enum.flat_map(module_analyses, fn analysis ->
      # Extract imports, aliases, and use statements
      # This would need to be implemented based on AST analysis
      []
    end)
    |> Enum.uniq()
  end
  
  defp calculate_estimated_overhead(module_analyses) do
    total_complexity = Enum.sum(Enum.map(module_analyses, & &1.complexity_score))
    instrumented_modules = Enum.count(module_analyses, &(&1.recommended_instrumentation != :minimal))
    
    # Estimate based on complexity and number of instrumented modules
    base_overhead = instrumented_modules * 0.001  # 0.1% per module
    complexity_overhead = total_complexity * 0.0001  # Based on complexity
    
    min(base_overhead + complexity_overhead, 0.05)  # Cap at 5%
  end
  
  defp match_supervisor_children(supervisor, workers) do
    # Match supervisor children specifications with actual worker modules
    Enum.filter(workers, fn worker ->
      # This would need more sophisticated matching based on actual children specs
      worker.file_path =~ supervisor.file_path
    end)
  end
  
  defp extract_genserver_calls(module_analyses) do
    # Extract GenServer call patterns from module ASTs
    # Implementation would analyze AST for GenServer.call/cast patterns
    []
  end
  
  defp extract_pubsub_usage(module_analyses) do
    # Extract Phoenix.PubSub usage patterns
    # Implementation would analyze AST for PubSub broadcast/subscribe patterns
    []
  end
  
  defp analyze_process_links(module_analyses) do
    # Analyze Process.link, spawn_link, and supervision relationships
    # Implementation would examine AST for process linking patterns
    []
  end
  
  defp in_critical_path?(module, project_analysis) do
    # Determine if module is in critical path based on supervision tree and call patterns
    # Supervisors and GenServers at top level are typically critical
    module.module_type in [:supervisor, :genserver]
  end
  
  defp enhance_to_full_tracing(base_type) do
    case base_type do
      :basic_state_tracking -> :full_state_tracking
      :request_lifecycle -> :request_lifecycle_with_db
      :minimal -> :performance_monitoring
      other -> other
    end
  end
  
  defp enhance_to_detailed_tracing(base_type) do
    case base_type do
      :minimal -> :basic_state_tracking
      other -> other
    end
  end
  
  defp simplify_instrumentation(base_type) do
    case base_type do
      :full_state_tracking -> :basic_state_tracking
      :request_lifecycle_with_db -> :request_lifecycle
      :performance_monitoring -> :minimal
      other -> other
    end
  end
  
  defp calculate_sampling_rate(priority) do
    case priority do
      :critical -> 1.0
      :high -> 0.8
      :medium -> 0.5
      :low -> 0.2
    end
  end
  
  defp calculate_performance_budget(priority) do
    case priority do
      :critical -> 1000  # 1000 microseconds
      :high -> 500
      :medium -> 200
      :low -> 50
    end
  end
  
  defp extract_all_modules(project_analysis) do
    project_analysis.project_structure.genservers ++
    project_analysis.project_structure.supervisors ++
    project_analysis.project_structure.phoenix_controllers ++
    project_analysis.project_structure.phoenix_liveviews ++
    project_analysis.project_structure.regular_modules
  end
  
  defp calculate_confidence(module_type, patterns) do
    base_confidence = case module_type do
      :genserver -> 0.9
      :supervisor -> 0.8
      :phoenix_controller -> 0.85
      :phoenix_liveview -> 0.85
      _ -> 0.6
    end
    
    # Adjust based on pattern recognition certainty
    pattern_confidence = if length(patterns.callbacks || []) > 0, do: 0.1, else: 0.0
    
    min(base_confidence + pattern_confidence, 1.0)
  end
  
  defp analyze_performance_patterns(ast) do
    # Analyze AST for performance-critical patterns
    has_enum_operations = ast_contains_pattern?(ast, {:Enum, :map})
    has_database_operations = ast_contains_pattern?(ast, {:Repo, :all})
    has_expensive_computations = calculate_computational_complexity(ast) > 5
    
    %{
      is_critical: has_enum_operations or has_database_operations or has_expensive_computations,
      enum_operations: has_enum_operations,
      database_operations: has_database_operations,
      computational_complexity: calculate_computational_complexity(ast)
    }
  end
  
  defp recommend_function_instrumentation(complexity, performance_indicators) do
    cond do
      performance_indicators.is_critical -> :performance_monitoring
      complexity.score > 8 -> :detailed_tracing  
      complexity.nesting_depth > 3 -> :basic_tracing
      true -> :minimal
    end
  end
  
  defp ast_contains_pattern?(ast, {module, function}) do
    Macro.prewalk(ast, false, fn
      {{:., _, [{:__aliases__, _, [^module]}, ^function]}, _, _}, _acc ->
        {true, true}
      node, acc -> {node, acc}
    end) |> elem(1)
  end
  
  defp calculate_computational_complexity(ast) do
    # Simple heuristic based on nested operations and function calls
    Macro.prewalk(ast, 0, fn
      {:case, _, _}, acc -> {true, acc + 2}
      {:if, _, _}, acc -> {true, acc + 1}  
      {:cond, _, _}, acc -> {true, acc + 2}
      {{:., _, _}, _, _}, acc -> {true, acc + 1}  # Function call
      node, acc -> {node, acc}
    end) |> elem(1)
  end
  
  defp extract_target({:__aliases__, _, module_parts}) do
    Module.concat(module_parts)
  end
  defp extract_target(atom) when is_atom(atom), do: atom
  defp extract_target(_), do: :unknown
  
  defp extract_message_pattern({:{}, _, [atom | _]}) when is_atom(atom), do: atom
  defp extract_message_pattern(atom) when is_atom(atom), do: atom
  defp extract_message_pattern(_), do: :unknown
  
  defp extract_topic_pattern({:<<>>, _, [topic_string]}) when is_binary(topic_string) do
    topic_string
  end
  defp extract_topic_pattern(_), do: :unknown
  
  defp extract_message_type({:{}, _, [type | _]}) when is_atom(type), do: type
  defp extract_message_type(_), do: :unknown
  
  defp extract_pubsub_server({:__aliases__, _, module_parts}) do
    Module.concat(module_parts)
  end
  defp extract_pubsub_server(_), do: :unknown
end
```

### 3.2 Pattern Recognition Module

#### Implementation Specification: `lib/elixir_scope/ai/pattern_recognizer.ex`

```elixir
defmodule ElixirScope.AI.PatternRecognizer do
  @moduledoc """
  Pattern recognition for Elixir code structures.
  
  Identifies common OTP patterns, Phoenix patterns, and architectural structures
  to inform instrumentation decisions.
  """
  
  @doc """
  Identifies the primary type of an Elixir module based on its AST.
  """
  def identify_module_type(ast) do
    cond do
      has_genserver_use?(ast) -> :genserver
      has_supervisor_use?(ast) -> :supervisor
      has_phoenix_controller_use?(ast) -> :phoenix_controller
      has_phoenix_liveview_use?(ast) -> :phoenix_liveview
      has_phoenix_channel_use?(ast) -> :phoenix_channel
      has_ecto_schema_use?(ast) -> :ecto_schema
      true -> :regular
    end
  end
  
  @doc """
  Extracts patterns and characteristics from module AST.
  """
  def extract_patterns(ast) do
    %{
      callbacks: extract_callbacks(ast),
      actions: extract_phoenix_actions(ast),
      events: extract_liveview_events(ast),
      children: extract_supervisor_children(ast),
      strategy: extract_supervisor_strategy(ast),
      database_interactions: has_database_interactions?(ast),
      message_patterns: extract_message_patterns(ast),
      pubsub_usage: extract_pubsub_patterns(ast)
    }
  end
  
  # GenServer pattern recognition
  
  defp has_genserver_use?(ast) do
    ast_contains_use?(ast, :GenServer)
  end
  
  defp extract_callbacks(ast) do
    genserver_callbacks = [:init, :handle_call, :handle_cast, :handle_info, :terminate, :code_change]
    liveview_callbacks = [:mount, :handle_event, :handle_info, :handle_params]
    
    defined_functions = extract_function_names(ast)
    
    callbacks = Enum.filter(genserver_callbacks ++ liveview_callbacks, fn callback ->
      callback in defined_functions
    end)
    
    callbacks
  end
  
  # Supervisor pattern recognition
  
  defp has_supervisor_use?(ast) do
    ast_contains_use?(ast, :Supervisor)
  end
  
  defp extract_supervisor_children(ast) do
    # Look for children list in init function
    Macro.prewalk(ast, [], fn
      {:def, _, [{:init, _, _}, body]}, acc ->
        children = extract_children_from_init(body)
        {children, children ++ acc}
      node, acc -> {node, acc}
    end) |> elem(1) |> List.flatten() |> Enum.uniq()
  end
  
  defp extract_supervisor_strategy(ast) do
    Macro.prewalk(ast, :one_for_one, fn
      {{:., _, [{:__aliases__, _, [:Supervisor]}, :init]}, _, [_children, [strategy: strategy]]}, _acc ->
        {strategy, strategy}
      node, acc -> {node, acc}
    end) |> elem(1)
  end
  
  # Phoenix pattern recognition
  
  defp has_phoenix_controller_use?(ast) do
    ast_contains_use_with_atom?(ast, :controller) or
    ast_contains_pattern?(ast, {:use, _, [{{:., _, [_, :controller]}, _, _}]})
  end
  
  defp has_phoenix_liveview_use?(ast) do
    ast_contains_use?(ast, Phoenix.LiveView)
  end
  
  defp has_phoenix_channel_use?(ast) do
    ast_contains_use?(ast, Phoenix.Channel)
  end
  
  defp extract_phoenix_actions(ast) do
    function_names = extract_function_names(ast)
    
    # Phoenix actions are typically public functions that take (conn, params)
    Enum.filter(function_names, fn name ->
      function_has_conn_params_signature?(ast, name)
    end)
  end
  
  defp extract_liveview_events(ast) do
    # Extract event names from handle_event functions
    Macro.prewalk(ast, [], fn
      {:def, _, [{:handle_event, _, [event_name | _]}, _]}, acc when is_binary(event_name) ->
        {event_name, [event_name | acc]}
      {:def, _, [{:handle_event, _, [{event_name, _, _} | _]}, _]}, acc when is_atom(event_name) ->
        {event_name, [Atom.to_string(event_name) | acc]}
      node, acc -> {node, acc}
    end) |> elem(1) |> Enum.uniq()
  end
  
  # Database interaction patterns
  
  defp has_database_interactions?(ast) do
    has_repo_calls?(ast) or has_ecto_queries?(ast)
  end
  
  defp has_repo_calls?(ast) do
    ast_contains_pattern?(ast, {{:., _, [{:__aliases__, _, [:Repo]}, _]}, _, _})
  end
  
  defp has_ecto_queries?(ast) do
    ast_contains_import?(ast, Ecto.Query)
  end
  
  defp has_ecto_schema_use?(ast) do
    ast_contains_use?(ast, Ecto.Schema)
  end
  
  # Message pattern extraction
  
  defp extract_message_patterns(ast) do
    Macro.prewalk(ast, [], fn
      # GenServer.call/cast patterns
      {{:., _, [{:__aliases__, _, [:GenServer]}, call_type]}, _, [_target, message]}, acc 
        when call_type in [:call, :cast] ->
        pattern = extract_message_structure(message)
        {message, [{call_type, pattern} | acc]}
        
      # send patterns
      {{:., _, [{:__aliases__, _, [:Process]}, :send]}, _, [_pid, message]}, acc ->
        pattern = extract_message_structure(message)
        {message, [{:send, pattern} | acc]}
        
      node, acc -> {node, acc}
    end) |> elem(1)
  end
  
  defp extract_pubsub_patterns(ast) do
    Macro.prewalk(ast, [], fn
      # Phoenix.PubSub.broadcast
      {{:., _, [{:__aliases__, _, [:Phoenix, :PubSub]}, :broadcast]}, _, [_pubsub, topic, message]}, acc ->
        topic_pattern = extract_topic_structure(topic)
        message_pattern = extract_message_structure(message)
        pattern = %{type: :broadcast, topic: topic_pattern, message: message_pattern}
        {pattern, [pattern | acc]}
        
      # Phoenix.PubSub.subscribe
      {{:., _, [{:__aliases__, _, [:Phoenix, :PubSub]}, :subscribe]}, _, [_pubsub, topic]}, acc ->
        topic_pattern = extract_topic_structure(topic)
        pattern = %{type: :subscribe, topic: topic_pattern}
        {pattern, [pattern | acc]}
        
      node, acc -> {node, acc}
    end) |> elem(1)
  end
  
  # Utility functions
  
  defp ast_contains_use?(ast, module) do
    Macro.prewalk(ast, false, fn
      {:use, _, [{:__aliases__, _, module_parts}]}, _acc when Module.concat(module_parts) == module ->
        {true, true}
      {:use, _, [^module]}, _acc when is_atom(module) ->
        {true, true}
      node, acc -> {node, acc}
    end) |> elem(1)
  end
  
  defp ast_contains_use_with_atom?(ast, atom) do
    Macro.prewalk(ast, false, fn
      {:use, _, [_, ^atom]}, _acc ->
        {true, true}
      node, acc -> {node, acc}
    end) |> elem(1)
  end
  
  defp ast_contains_pattern?(ast, pattern) do
    Macro.prewalk(ast, false, fn
      ^pattern, _acc -> {true, true}
      node, acc -> {node, acc}
    end) |> elem(1)
  end
  
  defp ast_contains_import?(ast, module) do
    Macro.prewalk(ast, false, fn
      {:import, _, [^module]}, _acc -> {true, true}
      {:import, _, [{:__aliases__, _, module_parts}]}, _acc when Module.concat(module_parts) == module ->
        {true, true}
      node, acc -> {node, acc}
    end) |> elem(1)
  end
  
  defp extract_function_names(ast) do
    Macro.prewalk(ast, [], fn
      {:def, _, [{name, _, _}, _]}, acc when is_atom(name) -> {name, [name | acc]}
      {:defp, _, [{name, _, _}, _]}, acc when is_atom(name) -> {name, [name | acc]}
      node, acc -> {node, acc}
    end) |> elem(1) |> Enum.uniq()
  end
  
  defp function_has_conn_params_signature?(ast, function_name) do
    Macro.prewalk(ast, false, fn
      {:def, _, [{^function_name, _, [_conn, _params]}, _]}, _acc -> {true, true}
      node, acc -> {node, acc}
    end) |> elem(1)
  end
  
  defp extract_children_from_init(body) do
    Macro.prewalk(body, [], fn
      {:=, _, [{:children, _, _}, children_list]}, acc ->
        children = extract_child_specs(children_list)
        {children_list, children ++ acc}
      node, acc -> {node, acc}
    end) |> elem(1)
  end
  
  defp extract_child_specs({:__block__, _, specs}) when is_list(specs) do
    Enum.map(specs, &extract_single_child_spec/1)
  end
  defp extract_child_specs(specs) when is_list(specs) do
    Enum.map(specs, &extract_single_child_spec/1)
  end
  defp extract_child_specs(_), do: []
  
  defp extract_single_child_spec({:__aliases__, _, module_parts}) do
    Module.concat(module_parts)
  end
  defp extract_single_child_spec({:{}, _, [module_ref | _]}) do
    extract_single_child_spec(module_ref)
  end
  defp extract_single_child_spec(_), do: :unknown
  
  defp extract_message_structure({:{}, _, [atom | _]}) when is_atom(atom) do
    atom
  end
  defp extract_message_structure(atom) when is_atom(atom) do
    atom
  end
  defp extract_message_structure(%{} = map) do
    :map_message
  end
  defp extract_message_structure(_) do
    :unknown
  end
  
  defp extract_topic_structure({:<<>>, _, [topic]}) when is_binary(topic) do
    topic
  end
  defp extract_topic_structure(topic) when is_binary(topic) do
    topic
  end
  defp extract_topic_structure(_) do
    :unknown
  end
end
```

---

## 4. Distributed System Support

### Overview
Distributed system support enables ElixirScope to trace and correlate events across multiple BEAM nodes, providing comprehensive visibility into distributed Elixir applications.

### 4.1 Multi-Node Event Correlation

#### Test Specification: `test/elixir_scope/distributed/multi_node_test.exs`

```elixir
defmodule ElixirScope.Distributed.MultiNodeTest do
  use ExUnit.Case
  
  alias ElixirScope.Distributed.NodeCoordinator
  alias ElixirScope.Distributed.EventSynchronizer
  alias ElixirScope.Storage.DataAccess
  
  @test_nodes [:node1@localhost, :node2@localhost, :node3@localhost]
  
  setup_all do
    # Start test nodes
    nodes = start_test_nodes(@test_nodes)
    
    # Start ElixirScope on all nodes
    for node <- nodes do
      :rpc.call(node, ElixirScope, :start, [])
    end
    
    # Configure distributed tracing
    NodeCoordinator.setup_cluster(nodes)
    
    on_exit(fn ->
      for node <- nodes do
        Node.stop(node)
      end
    end)
    
    {:ok, nodes: nodes}
  end
  
  describe "cross-node event correlation" do
    test "correlates GenServer call across nodes", %{nodes: [node1, node2, _node3]} do
      # Start GenServer on node2
      {:ok, server_pid} = :rpc.call(node2, GenServer, :start_link, [TestGenServer, %{}])
      
      # Make call from node1 to node2
      correlation_id = :rpc.call(node1, GenServer, :call, [server_pid, {:get, :key}])
      
      # Wait for event processing
      Process.sleep(200)
      
      # Verify events are correlated across nodes
      node1_events = :rpc.call(node1, DataAccess, :get_events_by_correlation, [correlation_id])
      node2_events = :rpc.call(node2, DataAccess, :get_events_by_correlation, [correlation_id])
      
      # Should have call event on node1
      assert find_event(node1_events, :genserver_call_start) != nil
      
      # Should have handle_call event on node2  
      assert find_event(node2_events, :genserver_handle_call_start) != nil
      assert find_event(node2_events, :genserver_handle_call_complete) != nil
      
      # Verify correlation IDs match
      all_events = node1_events ++ node2_events
      correlation_ids = Enum.map(all_events, & &1.correlation_id) |> Enum.uniq()
      assert length(correlation_ids) == 1
    end
    
    test "traces distributed process spawning", %{nodes: [node1, node2, _node3]} do
      # Spawn process on node2 from node1
      spawn_result = :rpc.call(node1, Node, :spawn, [node2, fn -> 
        Process.sleep(100)
        :ok
      end])
      
      Process.sleep(200)
      
      # Verify spawn events are captured
      node1_events = :rpc.call(node1, DataAccess, :get_events_by_type, [:process_spawn])
      node2_events = :rpc.call(node2, DataAccess, :get_events_by_type, [:process_start])
      
      assert length(node1_events) > 0
      assert length(node2_events) > 0
      
      # Verify parent-child relationship is tracked
      spawn_event = hd(node1_events)
      start_event = hd(node2_events)
      
      assert spawn_event.data.target_node == node2
      assert start_event.data.parent_node == node1
    end
    
    test "handles node disconnection gracefully", %{nodes: [node1, node2, node3]} do
      # Establish communication between nodes
      correlation_id = start_distributed_operation(node1, node2)
      
      # Disconnect node2
      :rpc.call(node2, Node, :disconnect, [node1])
      :rpc.call(node2, Node, :disconnect, [node3])
      
      Process.sleep(100)
      
      # Verify node1 detects disconnection
      node1_events = :rpc.call(node1, DataAccess, :get_events_by_type, [:node_disconnected])
      assert length(node1_events) > 0
      
      disconnect_event = hd(node1_events)
      assert disconnect_event.data.disconnected_node == node2
      assert disconnect_event.data.reason != nil
    end
  end
  
  describe "distributed event synchronization" do
    test "synchronizes events across nodes", %{nodes: [node1, node2, node3]} do
      # Generate events on all nodes
      correlation_id = generate_distributed_events(nodes)
      
      # Wait for synchronization
      Process.sleep(500)
      
      # Verify all nodes have complete event history
      for node <- nodes do
        events = :rpc.call(node, DataAccess, :get_events_by_correlation, [correlation_id])
        
        # Should have events from all nodes
        node_sources = Enum.map(events, & &1.node) |> Enum.uniq()
        assert length(node_sources) == 3
        
        # Should have proper ordering
        assert events_properly_ordered?(events)
      end
    end
    
    test "handles network partitions", %{nodes: [node1, node2, node3]} do
      # Create network partition: node1 isolated from node2,node3
      :rpc.call(node1, Node, :disconnect, [node2])
      :rpc.call(node1, Node, :disconnect, [node3])
      
      # Generate events during partition
      correlation_id1 = :rpc.call(node1, TestEventGenerator, :generate, [])
      correlation_id2 = :rpc.call(node2, TestEventGenerator, :generate, [])
      
      Process.sleep(100)
      
      # Verify events are stored locally during partition
      node1_events = :rpc.call(node1, DataAccess, :get_events_by_correlation, [correlation_id1])
      node2_events = :rpc.call(node2, DataAccess, :get_events_by_correlation, [correlation_id2])
      
      assert length(node1_events) > 0
      assert length(node2_events) > 0
      
      # Reconnect nodes
      :rpc.call(node1, Node, :connect, [node2])
      :rpc.call(node1, Node, :connect, [node3])
      
      # Wait for synchronization
      Process.sleep(300)
      
      # Verify events are eventually synchronized
      all_node1_events = :rpc.call(node1, DataAccess, :get_all_events, [])
      all_node2_events = :rpc.call(node2, DataAccess, :get_all_events, [])
      
      # Both nodes should have events from both correlation IDs
      node1_correlations = Enum.map(all_node1_events, & &1.correlation_id) |> Enum.uniq()
      node2_correlations = Enum.map(all_node2_events, & &1.correlation_id) |> Enum.uniq()
      
      assert correlation_id1 in node1_correlations
      assert correlation_id2 in node1_correlations
      assert correlation_id1 in node2_correlations
      assert correlation_id2 in node2_correlations
    end
  end
  
  describe "distributed performance impact" do
    test "measures cross-node tracing overhead", %{nodes: nodes} do
      # Baseline: measure without ElixirScope
      baseline_time = measure_distributed_operation_time(nodes, false)
      
      # With ElixirScope: measure with full tracing
      traced_time = measure_distributed_operation_time(nodes, true)
      
      # Verify overhead is acceptable
      overhead_percent = ((traced_time - baseline_time) / baseline_time) * 100
      assert overhead_percent < 10.0  # Less than 10% overhead
    end
    
    test "handles high-frequency distributed events", %{nodes: nodes} do
      # Generate high-frequency events across nodes
      start_time = System.monotonic_time(:millisecond)
      
      tasks = for node <- nodes do
        Task.async(fn ->
          :rpc.call(node, TestEventGenerator, :generate_high_frequency, [1000])
        end)
      end
      
      Task.await_many(tasks, 10_000)
      
      end_time = System.monotonic_time(:millisecond)
      duration = end_time - start_time
      
      # Verify performance is acceptable
      assert duration < 5000  # Should complete within 5 seconds
      
      # Verify no events were dropped
      total_events = Enum.reduce(nodes, 0, fn node, acc ->
        count = :rpc.call(node, DataAccess, :count_events, [])
        acc + count
      end)
      
      assert total_events >= 3000  # 1000 events per node
    end
  end

  # Helper functions
  
  defp start_test_nodes(node_names) do
    Enum.map(node_names, fn node_name ->
      {:ok, node} = :slave.start(:localhost, node_name, '-setcookie test_cookie')
      node
    end)
  end
  
  defp find_event(events, event_type) do
    Enum.find(events, &(&1.event_type == event_type))
  end
  
  defp start_distributed_operation(node1, node2) do
    correlation_id = ElixirScope.Utils.generate_correlation_id()
    
    # Start operation on node1 that calls node2
    :rpc.call(node1, TestDistributedOperation, :start, [node2, correlation_id])
    
    correlation_id
  end
  
  defp generate_distributed_events(nodes) do
    correlation_id = ElixirScope.Utils.generate_correlation_id()
    
    # Generate related events on each node
    for {node, index} <- Enum.with_index(nodes) do
      :rpc.call(node, TestEventGenerator, :generate_with_correlation, [correlation_id, index])
    end
    
    correlation_id
  end
  
  defp events_properly_ordered?(events) do
    # Verify events are ordered by timestamp
    timestamps = Enum.map(events, & &1.timestamp)
    timestamps == Enum.sort(timestamps)
  end
  
  defp measure_distributed_operation_time(nodes, with_tracing) do
    if with_tracing do
      for node <- nodes, do: :rpc.call(node, ElixirScope, :start, [])
    else
      for node <- nodes, do: :rpc.call(node, ElixirScope, :stop, [])
    end
    
    start_time = System.monotonic_time(:millisecond)
    
    # Perform standard distributed operation
    perform_distributed_benchmark(nodes)
    
    end_time = System.monotonic_time(:millisecond)
    end_time - start_time
  end
  
  defp perform_distributed_benchmark(nodes) do
    # Simulate typical distributed operations
    tasks = for node <- nodes do
      Task.async(fn ->
        :rpc.call(node, TestDistributedBenchmark, :run_operations, [100])
      end)
    end
    
    Task.await_many(tasks, 10_000)
  end
end
```

#### Implementation Specification: `lib/elixir_scope/distributed/node_coordinator.ex`

```elixir
defmodule ElixirScope.Distributed.NodeCoordinator do
  @moduledoc """
  Coordinates ElixirScope tracing across multiple BEAM nodes.
  
  Handles:
  - Node discovery and registration
  - Event synchronization across nodes
  - Distributed correlation ID management
  - Network partition handling
  - Cross-node query coordination
  """
  
  use GenServer
  
  alias ElixirScope.Distributed.EventSynchronizer
  alias ElixirScope.Distributed.GlobalClock
  alias ElixirScope.Storage.DataAccess
  
  defstruct [
    :local_node,
    :cluster_nodes,
    :sync_interval,
    :partition_detector,
    :global_clock
  ]
  
  @sync_interval_ms 1000
  @partition_check_interval_ms 5000
  
  ## Public API
  
  @doc """
  Starts the NodeCoordinator for the local node.
  """
  def start_link(opts \\ []) do
    GenServer.start_link(__MODULE__, opts, name: __MODULE__)
  end
  
  @doc """
  Sets up ElixirScope cluster with the given nodes.
  """
  def setup_cluster(nodes) do
    # Start coordinator on each node
    for node <- nodes do
      :rpc.call(node, __MODULE__, :start_link, [[cluster_nodes: nodes]])
    end
    
    # Wait for all nodes to be ready
    Process.sleep(100)
    
    # Initialize global clock synchronization
    GlobalClock.initialize_cluster(nodes)
    
    :ok
  end
  
  @doc """
  Registers a new node with the cluster.
  """
  def register_node(node) do
    GenServer.call(__MODULE__, {:register_node, node})
  end
  
  @doc """
  Gets all nodes currently in the cluster.
  """
  def get_cluster_nodes do
    GenServer.call(__MODULE__, :get_cluster_nodes)
  end
  
  @doc """
  Synchronizes events across all cluster nodes.
  """
  def sync_events do
    GenServer.call(__MODULE__, :sync_events)
  end
  
  @doc """
  Queries events across all nodes in the cluster.
  """
  def distributed_query(query_params) do
    GenServer.call(__MODULE__, {:distributed_query, query_params})
  end
  
  ## GenServer Implementation
  
  @impl true
  def init(opts) do
    cluster_nodes = Keyword.get(opts, :cluster_nodes, [node()])
    
    state = %__MODULE__{
      local_node: node(),
      cluster_nodes: cluster_nodes,
      sync_interval: @sync_interval_ms,
      partition_detector: nil,
      global_clock: nil
    }
    
    # Monitor node connections
    :net_kernel.monitor_nodes(true)
    
    # Schedule periodic synchronization
    schedule_sync()
    schedule_partition_check()
    
    {:ok, state}
  end
  
  @impl true
  def handle_call({:register_node, new_node}, _from, state) do
    if new_node not in state.cluster_nodes do
      updated_nodes = [new_node | state.cluster_nodes]
      updated_state = %{state | cluster_nodes: updated_nodes}
      
      # Notify other nodes about the new member
      notify_cluster_change(updated_nodes, {:node_joined, new_node})
      
      {:reply, :ok, updated_state}
    else
      {:reply, :already_registered, state}
    end
  end
  
  @impl true
  def handle_call(:get_cluster_nodes, _from, state) do
    {:reply, state.cluster_nodes, state}
  end
  
  @impl true
  def handle_call(:sync_events, _from, state) do
    result = perform_event_sync(state)
    {:reply, result, state}
  end
  
  @impl true
  def handle_call({:distributed_query, query_params}, _from, state) do
    results = execute_distributed_query(query_params, state)
    {:reply, results, state}
  end
  
  @impl true
  def handle_info(:periodic_sync, state) do
    perform_event_sync(state)
    schedule_sync()
    {:noreply, state}
  end
  
  @impl true
  def handle_info(:check_partitions, state) do
    updated_state = check_for_partitions(state)
    schedule_partition_check()
    {:noreply, updated_state}
  end
  
  @impl true
  def handle_info({:nodeup, node}, state) do
    ElixirScope.Capture.InstrumentationRuntime.report_node_event(
      :nodeup,
      node,
      System.monotonic_time()
    )
    
    # Attempt to add node to cluster if it's running ElixirScope
    case :rpc.call(node, __MODULE__, :register_node, [state.local_node]) do
      :ok -> 
        updated_state = %{state | cluster_nodes: [node | state.cluster_nodes]}
        {:noreply, updated_state}
      _ -> 
        {:noreply, state}
    end
  end
  
  @impl true
  def handle_info({:nodedown, node}, state) do
    ElixirScope.Capture.InstrumentationRuntime.report_node_event(
      :nodedown,
      node,
      System.monotonic_time()
    )
    
    # Remove node from cluster
    updated_nodes = List.delete(state.cluster_nodes, node)
    updated_state = %{state | cluster_nodes: updated_nodes}
    
    # Notify remaining nodes
    notify_cluster_change(updated_nodes, {:node_left, node})
    
    {:noreply, updated_state}
  end
  
  ## Private Functions
  
  defp schedule_sync do
    Process.send_after(self(), :periodic_sync, @sync_interval_ms)
  end
  
  defp schedule_partition_check do
    Process.send_after(self(), :check_partitions, @partition_check_interval_ms)
  end
  
  defp perform_event_sync(state) do
    try do
      EventSynchronizer.sync_with_cluster(state.cluster_nodes)
    rescue
      error ->
        Logger.warning("Event sync failed: #{inspect(error)}")
        {:error, error}
    end
  end
  
  defp execute_distributed_query(query_params, state) do
    # Execute query on all reachable nodes
    query_tasks = for node <- state.cluster_nodes do
      Task.async(fn ->
        case :rpc.call(node, DataAccess, :query_events, [query_params]) do
          {:badrpc, reason} -> {:error, {node, reason}}
          result -> {:ok, {node, result}}
        end
      end)
    end
    
    # Collect results with timeout
    results = Task.await_many(query_tasks, 5000)
    
    # Merge successful results
    successful_results = 
      results
      |> Enum.filter(&match?({:ok, _}, &1))
      |> Enum.map(fn {:ok, {node, events}} -> {node, events} end)
    
    # Combine and deduplicate events
    all_events = 
      successful_results
      |> Enum.flat_map(fn {_node, events} -> events end)
      |> Enum.uniq_by(& &1.id)
      |> Enum.sort_by(& &1.timestamp)
    
    {:ok, all_events}
  end
  
  defp check_for_partitions(state) do
    # Check connectivity to all cluster nodes
    reachable_nodes = Enum.filter(state.cluster_nodes, fn node ->
      node == state.local_node or Node.ping(node) == :pong
    end)
    
    unreachable_nodes = state.cluster_nodes -- reachable_nodes
    
    if length(unreachable_nodes) > 0 do
      ElixirScope.Capture.InstrumentationRuntime.report_partition_detected(
        unreachable_nodes,
        System.monotonic_time()
      )
    end
    
    %{state | cluster_nodes: reachable_nodes}
  end
  
  defp notify_cluster_change(nodes, change_event) do
    for node <- nodes do
      if node != node() do
        :rpc.cast(node, __MODULE__, :handle_cluster_change, [change_event])
      end
    end
  end
  
  def handle_cluster_change(change_event) do
    GenServer.cast(__MODULE__, {:cluster_change, change_event})
  end
end
```

### 4.2 Event Synchronization

#### Implementation Specification: `lib/elixir_scope/distributed/event_synchronizer.ex`

```elixir
defmodule ElixirScope.Distributed.EventSynchronizer do
  @moduledoc """
  Synchronizes events across distributed ElixirScope nodes.
  
  Handles:
  - Efficient event delta synchronization
  - Conflict resolution for overlapping events
  - Bandwidth optimization for large event sets
  - Eventual consistency guarantees
  """
  
  alias ElixirScope.Storage.DataAccess
  alias ElixirScope.Distributed.GlobalClock
  
  @sync_batch_size 1000
  @max_sync_age_ms 300_000  # 5 minutes
  
  @doc """
  Synchronizes events with all nodes in the cluster.
  """
  def sync_with_cluster(cluster_nodes) do
    local_node = node()
    other_nodes = Enum.reject(cluster_nodes, &(&1 == local_node))
    
    # Get local sync state
    last_sync_times = get_last_sync_times(other_nodes)
    
    # Sync with each node
    sync_results = for node <- other_nodes do
      sync_with_node(node, last_sync_times[node])
    end
    
    # Update sync timestamps
    now = GlobalClock.now()
    update_sync_timestamps(other_nodes, now)
    
    {:ok, sync_results}
  end
  
  @doc """
  Synchronizes events with a specific node.
  """
  def sync_with_node(target_node, last_sync_time \\ nil) do
    try do
      # Get events since last sync
      since_time = last_sync_time || (GlobalClock.now() - @max_sync_age_ms * 1_000_000)
      local_events = DataAccess.get_events_since(since_time)
      
      # Send our events and get theirs
      sync_request = %{
        from_node: node(),
        since_time: since_time,
        events: prepare_events_for_sync(local_events)
      }
      
      case :rpc.call(target_node, __MODULE__, :handle_sync_request, [sync_request]) do
        {:ok, remote_events} ->
          # Store remote events locally
          store_remote_events(remote_events, target_node)
          {:ok, length(remote_events)}
          
        {:error, reason} ->
          {:error, {target_node, reason}}
          
        {:badrpc, reason} ->
          {:error, {target_node, :unreachable, reason}}
      end
    rescue
      error -> {:error, {target_node, error}}
    end
  end
  
  @doc """
  Handles incoming synchronization requests from other nodes.
  """
  def handle_sync_request(%{from_node: from_node, since_time: since_time, events: remote_events}) do
    try do
      # Store remote events
      store_remote_events(remote_events, from_node)
      
      # Get our events since the requested time
      local_events = DataAccess.get_events_since(since_time)
      prepared_events = prepare_events_for_sync(local_events)
      
      {:ok, prepared_events}
    rescue
      error -> {:error, error}
    end
  end
  
  @doc """
  Forces a full synchronization with all cluster nodes.
  """
  def full_sync_with_cluster(cluster_nodes) do
    # Clear sync timestamps to force full sync
    clear_sync_timestamps()
    sync_with_cluster(cluster_nodes)
  end
  
  ## Private Functions
  
  defp get_last_sync_times(nodes) do
    Enum.reduce(nodes, %{}, fn node, acc ->
      last_sync = get_last_sync_time(node)
      Map.put(acc, node, last_sync)
    end)
  end
  
  defp get_last_sync_time(node) do
    case :ets.lookup(:elixir_scope_sync_state, {:last_sync, node}) do
      [{_, timestamp}] -> timestamp
      [] -> nil
    end
  end
  
  defp update_sync_timestamps(nodes, timestamp) do
    ensure_sync_table_exists()
    
    for node <- nodes do
      :ets.insert(:elixir_scope_sync_state, {{:last_sync, node}, timestamp})
    end
  end
  
  defp clear_sync_timestamps do
    ensure_sync_table_exists()
    :ets.delete_all_objects(:elixir_scope_sync_state)
  end
  
  defp ensure_sync_table_exists do
    case :ets.whereis(:elixir_scope_sync_state) do
      :undefined ->
        :ets.new(:elixir_scope_sync_state, [:named_table, :public, :set])
      _ -> 
        :ok
    end
  end
  
  defp prepare_events_for_sync(events) do
    # Compress events and remove large payloads for efficient transfer
    Enum.map(events, fn event ->
      %{
        id: event.id,
        timestamp: event.timestamp,
        wall_time: event.wall_time,
        node: event.node,
        pid: event.pid,
        correlation_id: event.correlation_id,
        event_type: event.event_type,
        data: compress_event_data(event.data),
        checksum: calculate_event_checksum(event)
      }
    end)
  end
  
  defp compress_event_data(data) when byte_size(:erlang.term_to_binary(data)) > 10000 do
    # Large data - compress or truncate
    compressed = :zlib.compress(:erlang.term_to_binary(data))
    {:compressed, compressed}
  end
  defp compress_event_data(data) do
    data
  end
  
  defp calculate_event_checksum(event) do
    event
    |> :erlang.term_to_binary()
    |> :erlang.md5()
    |> Base.encode16()
  end
  
  defp store_remote_events(remote_events, source_node) do
    # Process events in batches to avoid overwhelming the system
    remote_events
    |> Enum.chunk_every(@sync_batch_size)
    |> Enum.each(fn batch ->
      processed_batch = Enum.map(batch, fn event ->
        restore_event_from_sync(event, source_node)
      end)
      
      # Filter out events we already have
      new_events = Enum.reject(processed_batch, fn event ->
        DataAccess.event_exists?(event.id)
      end)
      
      # Store new events
      if length(new_events) > 0 do
        DataAccess.store_events(new_events)
      end
    end)
  end
  
  defp restore_event_from_sync(sync_event, source_node) do
    restored_data = case sync_event.data do
      {:compressed, compressed_data} ->
        compressed_data
        |> :zlib.uncompress()
        |> :erlang.binary_to_term()
      
      regular_data ->
        regular_data
    end
    
    %ElixirScope.Events.Event{
      id: sync_event.id,
      timestamp: sync_event.timestamp,
      wall_time: sync_event.wall_time,
      node: sync_event.node,
      pid: sync_event.pid,
      correlation_id: sync_event.correlation_id,
      event_type: sync_event.event_type,
      data: restored_data,
      sync_metadata: %{
        synced_from: source_node,
        synced_at: GlobalClock.now(),
        checksum: sync_event.checksum
      }
    }
  end
end
```

---

## 5. Production Integration Tests

### Overview
Production integration tests validate that ElixirScope works correctly with real Phoenix applications under realistic conditions, measuring performance impact and ensuring reliability.

### 5.1 Real Application Integration

#### Test Specification: `test/integration/production_phoenix_test.exs`

```elixir
defmodule ElixirScope.Integration.ProductionPhoenixTest do
  use ExUnit.Case
  
  alias ElixirScope.Storage.DataAccess
  alias ElixirScope.Phoenix.Integration
  
  @moduletag :integration
  @moduletag timeout: 60_000
  
  # Use a real Phoenix app for testing
  @test_app_path "test/fixtures/production_phoenix_app"
  
  setup_all do
    # Start the test Phoenix application
    {:ok, _} = start_test_phoenix_app()
    
    # Enable ElixirScope instrumentation
    :ok = ElixirScope.start(strategy: :full_trace)
    :ok = Integration.enable()
    
    # Wait for instrumentation to be active
    Process.sleep(1000)
    
    on_exit(fn ->
      ElixirScope.stop()
      stop_test_phoenix_app()
    end)
    
    :ok
  end
  
  describe "real Phoenix application tracing" do
    test "traces complete user registration flow" do
      # Simulate user registration through web interface
      registration_data = %{
        "user" => %{
          "email" => "test@example.com",
          "password" => "securepassword123",
          "name" => "Test User"
        }
      }
      
      # Make HTTP request to registration endpoint
      {:ok, response} = make_http_request(:post, "/api/users/register", registration_data)
      
      # Wait for async processing
      Process.sleep(500)
      
      # Extract correlation ID from response
      correlation_id = extract_correlation_id(response)
      
      # Verify complete trace was captured
      events = DataAccess.get_events_by_correlation(correlation_id)
      
      # Should capture the complete flow
      expected_events = [
        :phoenix_request_start,
        :phoenix_controller_entry,
        :ecto_query_start,         # User existence check
        :ecto_query_complete,
        :genserver_call_start,     # Password hashing service
        :genserver_handle_call_start,
        :genserver_handle_call_complete,
        :genserver_call_complete,
        :ecto_query_start,         # User creation
        :ecto_query_complete,
        :phoenix_pubsub_broadcast, # Welcome email notification
        :phoenix_controller_exit,
        :phoenix_request_complete
      ]
      
      verify_event_sequence(events, expected_events)
      
      # Verify data integrity
      request_event = find_event(events, :phoenix_request_start)
      assert request_event.data.path == "/api/users/register"
      assert request_event.data.method == "POST"
      
      # Verify database operations are captured
      db_events = filter_events(events, :ecto_query_start)
      assert length(db_events) >= 2  # At least existence check and creation
      
      # Verify GenServer interactions
      genserver_events = filter_events(events, :genserver_call_start)
      assert length(genserver_events) >= 1  # Password hashing service
    end
    
    test "traces LiveView real-time chat application" do
      # Connect to LiveView chat
      {:ok, view, _html} = connect_to_liveview("/chat/room/general")
      
      # Send several messages
      messages = [
        "Hello everyone!",
        "How is everyone doing?",
        "This is a test message"
      ]
      
      correlation_ids = for message <- messages do
        send_chat_message(view, message)
      end
      
      # Wait for all processing
      Process.sleep(1000)
      
      # Verify each message was fully traced
      for correlation_id <- correlation_ids do
        events = DataAccess.get_events_by_correlation(correlation_id)
        
        expected_flow = [
          :liveview_handle_event_start,
          :ecto_query_start,          # Save message to database
          :ecto_query_complete,
          :phoenix_pubsub_broadcast,  # Broadcast to other users
          :liveview_handle_event_complete
        ]
        
        verify_event_sequence(events, expected_flow)
      end
      
      # Verify PubSub correlation across users
      pubsub_events = DataAccess.get_events_by_type(:phoenix_pubsub_broadcast)
      assert length(pubsub_events) == length(messages)
      
      # Each broadcast should have corresponding receives
      for broadcast_event <- pubsub_events do
        receive_events = DataAccess.get_events_by_correlation(broadcast_event.correlation_id)
        receive_count = count_events(receive_events, :phoenix_pubsub_receive)
        assert receive_count > 0  # Should have at least one receiver
      end
    end
    
    test "traces background job processing" do
      # Trigger a background job that processes uploaded file
      file_data = generate_test_file_data()
      {:ok, response} = make_http_request(:post, "/api/files/upload", file_data)
      
      correlation_id = extract_correlation_id(response)
      
      # Wait for background processing to complete
      Process.sleep(2000)
      
      # Verify the complete async flow was traced
      events = DataAccess.get_events_by_correlation(correlation_id)
      
      # Should include:
      # 1. Initial upload request
      # 2. Job queuing
      # 3. Background worker processing
      # 4. File processing steps
      # 5. Database updates
      # 6. Completion notification
      
      job_queue_events = filter_events(events, :job_queued)
      assert length(job_queue_events) == 1
      
      worker_events = filter_events(events, :background_worker_start)
      assert length(worker_events) >= 1
      
      file_processing_events = filter_events(events, :file_processing)
      assert length(file_processing_events) >= 1
      
      # Verify correlation spans across processes
      pids = Enum.map(events, & &1.pid) |> Enum.uniq()
      assert length(pids) >= 3  # Web process, job queue, worker process
    end
  end
  
  describe "performance and reliability under load" do
    test "maintains performance under high load" do
      # Baseline: measure performance without ElixirScope
      ElixirScope.stop()
      baseline_metrics = run_load_test(requests: 1000, concurrency: 10)
      
      # With ElixirScope: measure performance with full tracing
      ElixirScope.start(strategy: :full_trace)
      traced_metrics = run_load_test(requests: 1000, concurrency: 10)
      
      # Verify acceptable overhead
      latency_overhead = calculate_overhead(traced_metrics.avg_latency, baseline_metrics.avg_latency)
      throughput_impact = calculate_impact(traced_metrics.throughput, baseline_metrics.throughput)
      
      assert latency_overhead < 15.0  # Less than 15% latency increase
      assert throughput_impact < 10.0  # Less than 10% throughput decrease
      
      # Verify no request failures due to tracing
      assert traced_metrics.error_rate <= baseline_metrics.error_rate
    end
    
    test "handles sustained load without memory leaks" do
      # Run sustained load for 5 minutes
      start_time = System.monotonic_time(:second)
      initial_memory = get_elixir_scope_memory_usage()
      
      # Continuous load generation
      load_task = Task.async(fn ->
        run_continuous_load(duration_seconds: 300)
      end)
      
      # Monitor memory usage every 30 seconds
      memory_samples = monitor_memory_usage(duration_seconds: 300, interval: 30)
      
      Task.await(load_task, 400_000)
      
      final_memory = get_elixir_scope_memory_usage()
      
      # Verify memory growth is bounded
      memory_growth = final_memory - initial_memory
      memory_growth_mb = memory_growth / (1024 * 1024)
      
      assert memory_growth_mb < 50  # Less than 50MB growth over 5 minutes
      
      # Verify no memory leak trend
      assert no_memory_leak_detected?(memory_samples)
    end
    
    test "recovers gracefully from failures" do
      # Inject various failure scenarios
      failure_scenarios = [
        :ets_table_corruption,
        :ring_buffer_overflow,
        :worker_process_crash,
        :correlation_engine_failure,
        :storage_disk_full
      ]
      
      for scenario <- failure_scenarios do
        # Reset system
        restart_elixir_scope()
        
        # Run normal operations
        baseline_correlation_id = generate_test_operations()
        
        # Inject failure
        inject_failure(scenario)
        
        # Continue operations
        post_failure_correlation_id = generate_test_operations()
        
        # Wait for recovery
        Process.sleep(1000)
        
        # Verify system recovered
        baseline_events = DataAccess.get_events_by_correlation(baseline_correlation_id)
        post_failure_events = DataAccess.get_events_by_correlation(post_failure_correlation_id)
        
        assert length(baseline_events) > 0
        assert length(post_failure_events) > 0
        
        # Verify no data corruption
        assert events_are_valid?(baseline_events)
        assert events_are_valid?(post_failure_events)
        
        # Verify correlation integrity maintained
        assert correlation_integrity_maintained?(baseline_events)
        assert correlation_integrity_maintained?(post_failure_events)
      end
    end
    
    test "handles concurrent debugging sessions" do
      # Start multiple debugging sessions simultaneously
      session_count = 5
      
      debug_sessions = for i <- 1..session_count do
        Task.async(fn ->
          # Each session traces different aspects
          strategy = case rem(i, 3) do
            0 -> :full_trace
            1 -> :performance_only
            2 -> :state_tracking_only
          end
          
          # Run session-specific operations
          run_debug_session(strategy, operations: 100)
        end)
      end
      
      results = Task.await_many(debug_sessions, 30_000)
      
      # Verify all sessions completed successfully
      assert Enum.all?(results, &match?({:ok, _}, &1))
      
      # Verify no cross-session interference
      total_events = DataAccess.count_all_events()
      expected_min_events = session_count * 50  # Conservative estimate
      
      assert total_events >= expected_min_events
      
      # Verify correlation IDs are unique across sessions
      all_correlation_ids = DataAccess.get_all_correlation_ids()
      unique_correlation_ids = Enum.uniq(all_correlation_ids)
      
      assert length(all_correlation_ids) == length(unique_correlation_ids)
    end
  end
  
  describe "data accuracy and completeness" do
    test "captures complete request lifecycle with zero data loss" do
      # Generate complex nested operations
      correlation_id = perform_complex_operation()
      
      Process.sleep(500)
      
      events = DataAccess.get_events_by_correlation(correlation_id)
      
      # Verify no events were dropped
      assert verify_complete_trace(events)
      
      # Verify event ordering is correct
      assert events_are_chronologically_ordered?(events)
      
      # Verify all GenServer state transitions are captured
      state_events = filter_events(events, :state_change)
      assert verify_state_transition_completeness(state_events)
      
      # Verify message send/receive correlation
      message_events = filter_events(events, [:message_send, :message_receive])
      assert verify_message_correlation_completeness(message_events)
    end
    
    test "accurately captures exception scenarios" do
      # Generate various types of exceptions
      exception_scenarios = [
        :function_exception,
        :genserver_crash,
        :database_timeout,
        :validation_error,
        :network_timeout
      ]
      
      for scenario <- exception_scenarios do
        correlation_id = trigger_exception_scenario(scenario)
        
        Process.sleep(200)
        
        events = DataAccess.get_events_by_correlation(correlation_id)
        
        # Should have error event
        error_events = filter_events(events, :error)
        assert length(error_events) >= 1
        
        error_event = hd(error_events)
        assert error_event.data.error_type != nil
        assert error_event.data.stacktrace != nil
        
        # Should have events leading up to error
        pre_error_events = filter_events_before(events, error_event.timestamp)
        assert length(pre_error_events) > 0
        
        # Verify error context is captured
        assert error_event.data.context != nil
      end
    end
    
    test "maintains data integrity across system restarts" do
      # Generate events
      pre_restart_correlation_id = generate_test_operations()
      
      Process.sleep(200)
      
      # Get events before restart
      pre_restart_events = DataAccess.get_events_by_correlation(pre_restart_correlation_id)
      pre_restart_count = DataAccess.count_all_events()
      
      # Restart ElixirScope
      ElixirScope.stop()
      Process.sleep(100)
      ElixirScope.start()
      
      # Verify data persisted
      persisted_events = DataAccess.get_events_by_correlation(pre_restart_correlation_id)
      persisted_count = DataAccess.count_all_events()
      
      assert length(persisted_events) == length(pre_restart_events)
      assert persisted_count >= pre_restart_count
      
      # Generate new events after restart
      post_restart_correlation_id = generate_test_operations()
      
      Process.sleep(200)
      
      # Verify new events are captured
      post_restart_events = DataAccess.get_events_by_correlation(post_restart_correlation_id)
      assert length(post_restart_events) > 0
      
      # Verify no data corruption
      assert events_are_valid?(persisted_events)
      assert events_are_valid?(post_restart_events)
    end
  end

  # Helper functions
  
  defp start_test_phoenix_app do
    # Start the test Phoenix application
    Application.ensure_all_started(:production_phoenix_app)
  end
  
  defp stop_test_phoenix_app do
    Application.stop(:production_phoenix_app)
  end
  
  defp make_http_request(method, path, data \\ %{}) do
    # Use HTTPoison or similar to make real HTTP requests
    url = "http://localhost:4000" <> path
    headers = [{"Content-Type", "application/json"}]
    body = Jason.encode!(data)
    
    case method do
      :get -> HTTPoison.get(url, headers)
      :post -> HTTPoison.post(url, body, headers)
      :put -> HTTPoison.put(url, body, headers)
      :delete -> HTTPoison.delete(url, headers)
    end
  end
  
  defp extract_correlation_id(response) do
    # Extract correlation ID from response headers or body
    case Enum.find(response.headers, fn {key, _} -> 
      String.downcase(key) == "x-correlation-id" 
    end) do
      {_, correlation_id} -> correlation_id
      nil -> 
        # Try to extract from response body
        case Jason.decode(response.body) do
          {:ok, %{"correlation_id" => id}} -> id
          _ -> nil
        end
    end
  end
  
  defp connect_to_liveview(path) do
    # Use Phoenix.LiveViewTest for real LiveView testing
    {:ok, view, html} = live(build_conn(), path)
    {view, html}
  end
  
  defp send_chat_message(view, message) do
    correlation_id = ElixirScope.Utils.generate_correlation_id()
    
    # Set correlation ID in process for tracking
    Process.put(:elixir_scope_correlation_id, correlation_id)
    
    # Send message through LiveView
    view
    |> form("#chat-form", message: %{text: message})
    |> render_submit()
    
    correlation_id
  end
  
  defp generate_test_file_data do
    %{
      "file" => %{
        "filename" => "test_document.pdf",
        "content_type" => "application/pdf",
        "content" => Base.encode64(File.read!("test/fixtures/sample.pdf"))
      }
    }
  end
  
  defp run_load_test(opts) do
    requests = Keyword.get(opts, :requests, 100)
    concurrency = Keyword.get(opts, :concurrency, 5)
    
    start_time = System.monotonic_time(:millisecond)
    
    # Create concurrent request tasks
    tasks = for _i <- 1..concurrency do
      Task.async(fn ->
        requests_per_task = div(requests, concurrency)
        
        for _j <- 1..requests_per_task do
          {time, response} = :timer.tc(fn ->
            make_http_request(:get, "/api/health")
          end)
          
          {time, response.status_code}
        end
      end)
    end
    
    # Collect all results
    all_results = Task.await_many(tasks, 60_000) |> List.flatten()
    
    end_time = System.monotonic_time(:millisecond)
    total_time = end_time - start_time
    
    # Calculate metrics
    response_times = Enum.map(all_results, &elem(&1, 0))
    status_codes = Enum.map(all_results, &elem(&1, 1))
    
    successful_requests = Enum.count(status_codes, &(&1 in 200..299))
    error_requests = length(all_results) - successful_requests
    
    %{
      total_requests: length(all_results),
      successful_requests: successful_requests,
      error_requests: error_requests,
      error_rate: error_requests / length(all_results),
      avg_latency: Enum.sum(response_times) / length(response_times) / 1000, # Convert to ms
      max_latency: Enum.max(response_times) / 1000,
      min_latency: Enum.min(response_times) / 1000,
      throughput: length(all_results) / (total_time / 1000), # Requests per second
      total_time: total_time
    }
  end
  
  defp calculate_overhead(traced_value, baseline_value) do
    ((traced_value - baseline_value) / baseline_value) * 100
  end
  
  defp calculate_impact(traced_value, baseline_value) do
    ((baseline_value - traced_value) / baseline_value) * 100
  end
  
  defp get_elixir_scope_memory_usage do
    # Get memory usage of ElixirScope processes
    elixir_scope_processes = Process.list()
    |> Enum.filter(fn pid ->
      case Process.info(pid, :dictionary) do
        {:dictionary, dict} -> 
          Keyword.get(dict, :"$initial_call") 
          |> to_string() 
          |> String.contains?("ElixirScope")
        _ -> false
      end
    end)
    
    Enum.reduce(elixir_scope_processes, 0, fn pid, acc ->
      case Process.info(pid, :memory) do
        {:memory, memory} -> acc + memory
        _ -> acc
      end
    end)
  end
  
  defp run_continuous_load(opts) do
    duration = Keyword.get(opts, :duration_seconds, 300)
    end_time = System.monotonic_time(:second) + duration
    
    Stream.repeatedly(fn ->
      if System.monotonic_time(:second) < end_time do
        make_http_request(:get, "/api/users/#{:rand.uniform(1000)}")
        Process.sleep(50)  # 20 requests per second
        :continue
      else
        :stop
      end
    end)
    |> Stream.take_while(&(&1 == :continue))
    |> Enum.to_list()
  end
  
  defp monitor_memory_usage(opts) do
    duration = Keyword.get(opts, :duration_seconds, 300)
    interval = Keyword.get(opts, :interval, 30)
    
    end_time = System.monotonic_time(:second) + duration
    
    Stream.unfold(System.monotonic_time(:second), fn current_time ->
      if current_time < end_time do
        memory = get_elixir_scope_memory_usage()
        Process.sleep(interval * 1000)
        next_time = System.monotonic_time(:second)
        {{current_time, memory}, next_time}
      else
        nil
      end
    end)
    |> Enum.to_list()
  end
  
  defp no_memory_leak_detected?(memory_samples) do
    # Simple linear regression to detect memory leak trend
    if length(memory_samples) < 3 do
      true
    else
      {times, memories} = Enum.unzip(memory_samples)
      
      # Calculate trend
      n = length(times)
      sum_x = Enum.sum(times)
      sum_y = Enum.sum(memories)
      sum_xy = Enum.zip(times, memories) |> Enum.map(fn {x, y} -> x * y end) |> Enum.sum()
      sum_x2 = Enum.map(times, &(&1 * &1)) |> Enum.sum()
      
      slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)
      
      # Memory leak if slope indicates significant upward trend
      # Consider it a leak if memory grows more than 1MB per minute
      slope < 1024 * 1024 / 60
    end
  end
  
  defp restart_elixir_scope do
    ElixirScope.stop()
    Process.sleep(100)
    ElixirScope.start()
    Process.sleep(200)
  end
  
  defp generate_test_operations do
    correlation_id = ElixirScope.Utils.generate_correlation_id()
    
    # Set correlation ID for tracking
    Process.put(:elixir_scope_correlation_id, correlation_id)
    
    # Perform various operations
    make_http_request(:get, "/api/users")
    make_http_request(:post, "/api/posts", %{"post" => %{"title" => "Test", "content" => "Content"}})
    make_http_request(:get, "/api/posts/1")
    
    correlation_id
  end
  
  defp inject_failure(scenario) do
    case scenario do
      :ets_table_corruption ->
        # Simulate ETS table corruption
        :ets.delete_all_objects(:elixir_scope_events)
        
      :ring_buffer_overflow ->
        # Fill ring buffer to capacity
        buffer = get_main_ring_buffer()
        for _i <- 1..10000 do
          ElixirScope.Capture.RingBuffer.write(buffer, create_test_event())
        end
        
      :worker_process_crash ->
        # Crash async writer workers
        workers = get_async_writer_workers()
        Enum.each(workers, &Process.exit(&1, :kill))
        
      :correlation_engine_failure ->
        # Crash correlation engine
        case Process.whereis(ElixirScope.Capture.EventCorrelator) do
          nil -> :ok
          pid -> Process.exit(pid, :kill)
        end
        
      :storage_disk_full ->
        # Simulate disk full by making writes fail
        mock_disk_full_condition()
    end
  end
  
  defp verify_event_sequence(events, expected_sequence) do
    event_types = Enum.map(events, & &1.event_type)
    
    # Verify all expected events are present
    for expected_type <- expected_sequence do
      assert expected_type in event_types, 
        "Expected event #{expected_type} not found in #{inspect(event_types)}"
    end
    
    # Verify rough ordering (some events may be async)
    key_events = Enum.filter(events, &(&1.event_type in expected_sequence))
    assert length(key_events) >= length(expected_sequence) * 0.8  # Allow for some variation
  end
  
  defp find_event(events, event_type) do
    Enum.find(events, &(&1.event_type == event_type))
  end
  
  defp filter_events(events, event_type) when is_atom(event_type) do
    Enum.filter(events, &(&1.event_type == event_type))
  end
  
  defp filter_events(events, event_types) when is_list(event_types) do
    Enum.filter(events, &(&1.event_type in event_types))
  end
  
  defp count_events(events, event_type) do
    length(filter_events(events, event_type))
  end
  
  defp filter_events_before(events, timestamp) do
    Enum.filter(events, &(&1.timestamp < timestamp))
  end
  
  defp events_are_valid?(events) do
    Enum.all?(events, fn event ->
      event.id != nil and
      event.timestamp != nil and
      event.event_type != nil and
      is_pid(event.pid)
    end)
  end
  
  defp events_are_chronologically_ordered?(events) do
    timestamps = Enum.map(events, & &1.timestamp)
    timestamps == Enum.sort(timestamps)
  end
  
  defp correlation_integrity_maintained?(events) do
    correlation_ids = Enum.map(events, & &1.correlation_id) |> Enum.uniq()
    
    # All events should have the same correlation ID
    length(correlation_ids) <= 1
  end
  
  defp verify_complete_trace(events) do
    # Verify that for every function entry, there's a corresponding exit
    entry_events = filter_events(events, :function_entry)
    exit_events = filter_events(events, :function_exit)
    
    entry_call_ids = Enum.map(entry_events, & &1.data.call_id) |> MapSet.new()
    exit_call_ids = Enum.map(exit_events, & &1.data.call_id) |> MapSet.new()
    
    # All entries should have corresponding exits (allowing for some async operations)
    missing_exits = MapSet.difference(entry_call_ids, exit_call_ids)
    MapSet.size(missing_exits) <= MapSet.size(entry_call_ids) * 0.1  # Allow 10% async
  end
  
  defp verify_state_transition_completeness(state_events) do
    # Group by PID and verify state transitions are logical
    events_by_pid = Enum.group_by(state_events, & &1.pid)
    
    Enum.all?(events_by_pid, fn {_pid, pid_events} ->
      # Sort by timestamp
      sorted_events = Enum.sort_by(pid_events, & &1.timestamp)
      
      # Verify each state change has both before and after
      Enum.all?(sorted_events, fn event ->
        event.data.old_state != nil and event.data.new_state != nil
      end)
    end)
  end
  
  defp verify_message_correlation_completeness(message_events) do
    send_events = filter_events(message_events, :message_send)
    receive_events = filter_events(message_events, :message_receive)
    
    send_message_ids = Enum.map(send_events, & &1.data.message_id) |> MapSet.new()
    receive_message_ids = Enum.map(receive_events, & &1.data.message_id) |> MapSet.new()
    
    # Most sends should have corresponding receives
    matched_messages = MapSet.intersection(send_message_ids, receive_message_ids)
    match_ratio = MapSet.size(matched_messages) / MapSet.size(send_message_ids)
    
    match_ratio >= 0.8  # 80% of messages should be matched
  end
  
  defp perform_complex_operation do
    correlation_id = ElixirScope.Utils.generate_correlation_id()
    Process.put(:elixir_scope_correlation_id, correlation_id)
    
    # Complex operation involving multiple components
    {:ok, response} = make_http_request(:post, "/api/complex_operation", %{
      "data" => %{
        "type" => "batch_processing",
        "items" => for i <- 1..10, do: %{"id" => i, "value" => "item_#{i}"}
      }
    })
    
    correlation_id
  end
  
  defp trigger_exception_scenario(scenario) do
    correlation_id = ElixirScope.Utils.generate_correlation_id()
    Process.put(:elixir_scope_correlation_id, correlation_id)
    
    endpoint = case scenario do
      :function_exception -> "/api/test/function_exception"
      :genserver_crash -> "/api/test/genserver_crash"
      :database_timeout -> "/api/test/database_timeout"
      :validation_error -> "/api/test/validation_error"
      :network_timeout -> "/api/test/network_timeout"
    end
    
    # Make request that will trigger exception (expect it to fail)
    make_http_request(:post, endpoint, %{})
    
    correlation_id
  end
  
  defp run_debug_session(strategy, opts) do
    operations = Keyword.get(opts, :operations, 50)
    
    # Configure ElixirScope for this session
    ElixirScope.update_instrumentation(strategy: strategy)
    
    # Perform operations
    results = for _i <- 1..operations do
      correlation_id = generate_test_operations()
      {correlation_id, :ok}
    end
    
    {:ok, results}
  end
  
  # Mock helper functions (these would be implemented based on actual system)
  
  defp get_main_ring_buffer do
    # Get the main ring buffer instance
    Application.get_env(:elixir_scope, :main_buffer)
  end
  
  defp get_async_writer_workers do
    # Get PIDs of async writer workers
    case Process.whereis(ElixirScope.Capture.AsyncWriterPool) do
      nil -> []
      pool_pid -> 
        ElixirScope.Capture.AsyncWriterPool.get_worker_pids(pool_pid)
    end
  end
  
  defp create_test_event do
    %ElixirScope.Events.FunctionExecution{
      id: ElixirScope.Utils.generate_id(),
      timestamp: ElixirScope.Utils.monotonic_timestamp(),
      module: TestModule,
      function: :test_function,
      event_type: :call
    }
  end
  
  defp mock_disk_full_condition do
    # Mock disk full by intercepting file writes
    # This would require more sophisticated mocking
    :ok
  end
end
```

## 6. Implementation Priority and Timeline

### Overview
This section outlines the recommended implementation order and timeline for building the missing foundational components, ensuring each phase delivers incremental value while building toward the complete vision.

### 6.1 Implementation Phases

#### Phase 1: AST Transformation Foundation (Weeks 1-4)
**Priority**: Critical - This is the bridge between infrastructure and value delivery

**Week 1-2: Core AST Transformation**
- Implement `ElixirScope.Compiler.MixTask` with basic compiler integration
- Build `ElixirScope.AST.Transformer` with function instrumentation
- Create `ElixirScope.AST.InjectorHelpers` for code generation
- **Deliverable**: Simple functions can be automatically instrumented

**Week 3-4: GenServer and Pattern Support**
- Add GenServer callback instrumentation
- Implement pattern recognition for common OTP structures
- Add exception handling and state capture
- **Deliverable**: GenServers are automatically instrumented with state tracking

**Success Criteria**:
- Any Elixir module can be compiled with basic instrumentation
- Instrumented code maintains semantic equivalence
- GenServer state changes are captured automatically
- Performance overhead < 5% for basic instrumentation

#### Phase 2: Phoenix Integration Layer (Weeks 5-8)
**Priority**: High - Proves real-world value with actual applications

**Week 5-6: HTTP Request Lifecycle**
- Implement `ElixirScope.Phoenix.Integration` with Telemetry handlers
- Add request/response correlation across the Phoenix stack
- Build controller action instrumentation
- **Deliverable**: Complete HTTP request traces from router to response

**Week 7-8: LiveView and Real-time Features**
- Add LiveView mount, event, and state change tracing
- Implement Channel message flow correlation
- Build PubSub broadcast/receive tracking
- **Deliverable**: Real-time features are fully traced with event correlation

**Success Criteria**:
- Complete Phoenix request lifecycle is traced automatically
- LiveView state changes are correlated with triggering events
- WebSocket/Channel messages are tracked across processes
- Integration works with existing Phoenix applications

#### Phase 3: AI Code Analysis Engine (Weeks 9-12)
**Priority**: Medium-High - Enables intelligent automation

**Week 9-10: Pattern Recognition**
- Implement `ElixirScope.AI.PatternRecognizer` for OTP patterns
- Build `ElixirScope.AI.ComplexityAnalyzer` for code complexity
- Create rule-based analysis for common Elixir structures
- **Deliverable**: AI can identify GenServers, Supervisors, Phoenix components

**Week 11-12: Instrumentation Planning**
- Implement `ElixirScope.AI.InstrumentationPlanner` 
- Build project-wide analysis and recommendation system
- Create performance impact estimation
- **Deliverable**: AI generates optimal instrumentation plans automatically

**Success Criteria**:
- AI correctly identifies 90%+ of OTP patterns in test projects
- Generated instrumentation plans are sensible and targeted
- Performance impact estimates are within 20% of actual measurements
- Plans adapt based on project complexity and structure

#### Phase 4: Distributed System Support (Weeks 13-16)
**Priority**: Medium - Important for production but not MVP

**Week 13-14: Multi-Node Coordination**
- Implement `ElixirScope.Distributed.NodeCoordinator`
- Build cluster discovery and node registration
- Add basic cross-node event correlation
- **Deliverable**: Events can be correlated across multiple nodes

**Week 15-16: Event Synchronization**
- Implement `ElixirScope.Distributed.EventSynchronizer`
- Build efficient delta synchronization
- Add network partition handling
- **Deliverable**: Distributed traces remain consistent across network issues

**Success Criteria**:
- GenServer calls across nodes are properly correlated
- System handles node disconnections gracefully
- Event synchronization has minimal network overhead
- Distributed queries return complete results

### 6.2 Testing Strategy by Phase

#### Phase 1 Testing: AST Transformation
```elixir
# Critical tests to implement first
test "preserves semantic equivalence after instrumentation"
test "handles complex function patterns correctly"
test "instruments GenServer callbacks with state capture"
test "performance overhead is acceptable"
```

#### Phase 2 Testing: Phoenix Integration
```elixir
# Integration tests with real Phoenix apps
test "traces complete HTTP request lifecycle"
test "correlates LiveView events with state changes"
test "tracks Channel message flows"
test "works with existing Phoenix applications"
```

#### Phase 3 Testing: AI Analysis
```elixir
# AI accuracy and effectiveness tests
test "correctly identifies OTP patterns in real projects"
test "generates effective instrumentation plans"
test "estimates performance impact accurately"
test "adapts to different project structures"
```

#### Phase 4 Testing: Distributed Systems
```elixir
# Distributed system reliability tests
test "correlates events across multiple nodes"
test "handles network partitions gracefully"
test "synchronizes events efficiently"
test "maintains consistency under failures"
```

### 6.3 Success Metrics and Validation

#### Phase 1 Metrics
- **Functional**: 95% of Elixir language constructs supported
- **Performance**: <5% overhead for basic instrumentation
- **Reliability**: Zero semantic changes to instrumented code
- **Coverage**: GenServer, Supervisor, and regular module support

#### Phase 2 Metrics
- **Integration**: Works with Phoenix 1.6+ applications
- **Completeness**: 90% of request lifecycle events captured
- **Correlation**: 95% of related events properly linked
- **Performance**: <10% impact on Phoenix request latency

#### Phase 3 Metrics
- **Accuracy**: 90% pattern recognition accuracy on test projects
- **Effectiveness**: AI plans perform within 20% of manual optimization
- **Adaptability**: Works across different project sizes and structures
- **Automation**: <5 minutes to analyze and instrument typical project

#### Phase 4 Metrics
- **Reliability**: 99% event correlation success across nodes
- **Resilience**: Handles node failures without data loss
- **Efficiency**: <1% network overhead for synchronization
- **Scalability**: Supports 5+ node clusters effectively

### 6.4 Risk Mitigation

#### High-Risk Areas
1. **AST Transformation Complexity**: Elixir's macro system is complex
   - *Mitigation*: Start with simple cases, expand gradually
   - *Fallback*: Manual instrumentation for complex macros

2. **Performance Impact**: Instrumentation could slow applications significantly
   - *Mitigation*: Continuous performance testing, sampling strategies
   - *Fallback*: Configurable instrumentation levels

3. **Phoenix Integration Breaking Changes**: Phoenix APIs may change
   - *Mitigation*: Use stable Telemetry events, version compatibility matrix
   - *Fallback*: Multiple Phoenix version support

#### Medium-Risk Areas
1. **AI Analysis Accuracy**: Rule-based AI may miss edge cases
   - *Mitigation*: Extensive test suite, user feedback loop
   - *Fallback*: Manual instrumentation overrides

2. **Distributed System Complexity**: Network issues, timing problems
   - *Mitigation*: Robust testing, graceful degradation
   - *Fallback*: Single-node operation mode

### 6.5 Resource Requirements

#### Development Team
- **Lead Engineer**: Experienced with Elixir/OTP, AST manipulation
- **Phoenix Expert**: Deep Phoenix/LiveView knowledge
- **Testing Specialist**: Focus on integration and performance testing
- **DevOps Engineer**: Distributed systems and deployment

#### Infrastructure
- **Test Applications**: Multiple Phoenix apps of varying complexity
- **Multi-Node Testing**: Cluster testing environment
- **Performance Testing**: Load testing infrastructure
- **CI/CD Pipeline**: Automated testing across Elixir/Phoenix versions

### 6.6 Delivery Timeline

```
Weeks 1-4:  AST Transformation Foundation
              Week 1-2: Core transformation engine
              Week 3-4: GenServer and patterns

Weeks 5-8:  Phoenix Integration Layer  
              Week 5-6: HTTP lifecycle tracing
              Week 7-8: LiveView and real-time

Weeks 9-12: AI Code Analysis Engine
              Week 9-10: Pattern recognition
              Week 11-12: Instrumentation planning

Weeks 13-16: Distributed System Support
              Week 13-14: Multi-node coordination  
              Week 15-16: Event synchronization

Weeks 17-20: Integration and Optimization
              Week 17-18: End-to-end testing
              Week 19-20: Performance optimization
```

### 6.7 Definition of Done

Each phase is considered complete when:

1. **All planned functionality is implemented and tested**
2. **Performance targets are met or exceeded**
3. **Integration tests pass with real applications**
4. **Documentation is complete and accurate**
5. **Code coverage exceeds 90% for new components**
6. **Performance regression tests are in place**
7. **User acceptance criteria are validated**

---

## 7. Advanced Implementation Details

### 7.1 AST Transformation Edge Cases

#### Handling Complex Macro Scenarios

```elixir
defmodule ElixirScope.AST.MacroHandler do
  @moduledoc """
  Specialized handling for complex macro scenarios that standard
  AST transformation cannot handle safely.
  """
  
  @doc """
  Identifies macros that require special handling.
  """
  def requires_special_handling?(ast) do
    case ast do
      # Phoenix.Controller macros
      {:use, _, [{{:., _, [{:__aliases__, _, [:Phoenix]}, :Controller]}, _, _}]} -> true
      
      # Ecto.Schema with complex field definitions
      {:use, _, [{:__aliases__, _, [:Ecto, :Schema]}]} -> check_complex_schema(ast)
      
      # Custom DSL macros
      {macro_name, _, _} when macro_name in [:defstruct, :defprotocol] -> true
      
      _ -> false
    end
  end
  
  @doc """
  Safely transforms macro-heavy code by preserving macro boundaries.
  """
  def safe_macro_transform(ast, plan) do
    case identify_macro_type(ast) do
      :phoenix_controller -> transform_phoenix_controller_macro(ast, plan)
      :ecto_schema -> transform_ecto_schema_macro(ast, plan)
      :custom_dsl -> transform_custom_dsl_macro(ast, plan)
      :unknown -> {:error, :unsupported_macro}
    end
  end
  
  defp transform_phoenix_controller_macro(ast, plan) do
    # Phoenix controllers need special handling for action macros
    Macro.prewalk(ast, fn
      # Preserve plug macros but instrument actions
      {:plug, _, _} = plug_node -> plug_node
      
      # Transform action functions
      {:def, meta, [{action_name, _, args}, body]} = action_def 
        when action_name not in [:__using__, :__before_compile__] ->
        
        if should_instrument_action?(action_name, plan) do
          transformed_body = inject_phoenix_action_instrumentation(body, action_name, args)
          {:def, meta, [{action_name, [], args}, transformed_body]}
        else
          action_def
        end
        
      node -> node
    end)
  end
  
  defp transform_ecto_schema_macro(ast, plan) do
    # Ecto schemas need careful handling to preserve field definitions
    Macro.prewalk(ast, fn
      # Preserve schema macro calls
      {:schema, _, _} = schema_node -> schema_node
      {:embedded_schema, _, _} = embedded_node -> embedded_node
      
      # Transform changeset functions
      {:def, meta, [{:changeset, _, args}, body]} = changeset_def ->
        if plan.instrument_changesets do
          transformed_body = inject_changeset_instrumentation(body, args)
          {:def, meta, [{:changeset, [], args}, transformed_body]}
        else
          changeset_def
        end
        
      node -> node
    end)
  end
  
  defp inject_phoenix_action_instrumentation(body, action_name, args) do
    quote do
      correlation_id = ElixirScope.Capture.InstrumentationRuntime.report_phoenix_action_entry(
        unquote(action_name),
        unquote(args)
      )
      
      try do
        result = unquote(body)
        
        ElixirScope.Capture.InstrumentationRuntime.report_phoenix_action_exit(
          correlation_id,
          result
        )
        
        result
      rescue
        error ->
          ElixirScope.Capture.InstrumentationRuntime.report_phoenix_action_error(
            correlation_id,
            error,
            __STACKTRACE__
          )
          
          reraise error, __STACKTRACE__
      end
    end
  end
end
```

#### Performance-Critical Path Optimization

```elixir
defmodule ElixirScope.AST.PerformanceOptimizer do
  @moduledoc """
  Optimizes AST transformations for performance-critical paths.
  """
  
  def optimize_hot_path_instrumentation(ast, performance_profile) do
    case performance_profile.criticality do
      :ultra_critical -> apply_minimal_instrumentation(ast)
      :high -> apply_selective_instrumentation(ast, performance_profile)
      :medium -> apply_standard_instrumentation(ast)
      :low -> apply_comprehensive_instrumentation(ast)
    end
  end
  
  defp apply_minimal_instrumentation(ast) do
    # Only instrument entry/exit, no argument capture
    quote do
      start_time = System.monotonic_time(:nanosecond)
      
      try do
        result = unquote(ast)
        end_time = System.monotonic_time(:nanosecond)
        
        ElixirScope.Capture.InstrumentationRuntime.report_minimal_trace(
          __MODULE__,
          unquote(extract_function_name(ast)),
          end_time - start_time
        )
        
        result
      rescue
        error ->
          ElixirScope.Capture.InstrumentationRuntime.report_error_minimal(
            __MODULE__,
            unquote(extract_function_name(ast)),
            error
          )
          reraise error, __STACKTRACE__
      end
    end
  end
  
  defp apply_selective_instrumentation(ast, profile) do
    # Instrument based on specific performance characteristics
    instrumentation_calls = build_selective_instrumentation(profile)
    
    quote do
      unquote_splicing(instrumentation_calls.entry)
      
      result = unquote(ast)
      
      unquote_splicing(instrumentation_calls.exit)
      
      result
    end
  end
end
```

### 7.2 Phoenix Integration Advanced Scenarios

#### Complex LiveView State Tracking

```elixir
defmodule ElixirScope.Phoenix.LiveViewTracker do
  @moduledoc """
  Advanced LiveView state tracking that handles complex scenarios.
  """
  
  def instrument_liveview_module(ast, plan) do
    Macro.prewalk(ast, fn
      # Handle mount with complex assigns
      {:def, meta, [{:mount, _, args}, body]} = mount_def ->
        transformed_body = instrument_mount_with_assigns_tracking(body, args)
        {:def, meta, [{:mount, [], args}, transformed_body]}
      
      # Handle handle_event with state diffs
      {:def, meta, [{:handle_event, _, [event_name, params, socket]}, body]} = event_def ->
        transformed_body = instrument_handle_event_with_diff(body, event_name, params, socket)
        {:def, meta, [{:handle_event, [], [event_name, params, socket]}, transformed_body]}
        
      # Handle handle_info with message correlation
      {:def, meta, [{:handle_info, _, [message, socket]}, body]} = info_def ->
        transformed_body = instrument_handle_info_with_correlation(body, message, socket)
        {:def, meta, [{:handle_info, [], [message, socket]}, transformed_body]}
        
      node -> node
    end)
  end
  
  defp instrument_mount_with_assigns_tracking(body, [params, session, socket]) do
    quote do
      correlation_id = ElixirScope.Utils.generate_correlation_id()
      
      # Capture initial state
      ElixirScope.Capture.InstrumentationRuntime.report_liveview_mount_start(
        correlation_id,
        __MODULE__,
        unquote(params),
        unquote(session)
      )
      
      # Execute original mount
      result = unquote(body)
      
      # Capture final socket state
      case result do
        {:ok, final_socket} ->
          ElixirScope.Capture.InstrumentationRuntime.report_liveview_mount_complete(
            correlation_id,
            final_socket.assigns,
            extract_socket_metadata(final_socket)
          )
          
        {:ok, final_socket, opts} ->
          ElixirScope.Capture.InstrumentationRuntime.report_liveview_mount_complete(
            correlation_id,
            final_socket.assigns,
            extract_socket_metadata(final_socket),
            opts
          )
          
        other -> other
      end
      
      result
    end
  end
  
  defp instrument_handle_event_with_diff(body, event_name, params, socket) do
    quote do
      # Capture event start and current state
      correlation_id = ElixirScope.Utils.generate_correlation_id()
      initial_assigns = unquote(socket).assigns
      
      ElixirScope.Capture.InstrumentationRuntime.report_liveview_event_start(
        correlation_id,
        unquote(event_name),
        unquote(params),
        initial_assigns
      )
      
      # Execute event handler
      result = unquote(body)
      
      # Calculate and report state diff
      case result do
        {:noreply, new_socket} ->
          assigns_diff = calculate_assigns_diff(initial_assigns, new_socket.assigns)
          
          ElixirScope.Capture.InstrumentationRuntime.report_liveview_event_complete(
            correlation_id,
            assigns_diff,
            extract_socket_changes(unquote(socket), new_socket)
          )
          
        {:reply, reply_data, new_socket} ->
          assigns_diff = calculate_assigns_diff(initial_assigns, new_socket.assigns)
          
          ElixirScope.Capture.InstrumentationRuntime.report_liveview_event_complete(
            correlation_id,
            assigns_diff,
            extract_socket_changes(unquote(socket), new_socket),
            reply_data
          )
          
        other -> other
      end
      
      result
    end
  end
  
  def calculate_assigns_diff(old_assigns, new_assigns) do
    added = Map.drop(new_assigns, Map.keys(old_assigns))
    removed = Map.drop(old_assigns, Map.keys(new_assigns))
    
    changed = 
      Map.keys(old_assigns)
      |> Enum.filter(&Map.has_key?(new_assigns, &1))
      |> Enum.filter(fn key -> old_assigns[key] != new_assigns[key] end)
      |> Enum.into(%{}, fn key -> {key, {old_assigns[key], new_assigns[key]}} end)
    
    %{
      added: added,
      removed: Map.keys(removed),
      changed: changed
    }
  end
end
```

#### Channel Message Flow Correlation

```elixir
defmodule ElixirScope.Phoenix.ChannelTracker do
  @moduledoc """
  Tracks message flows through Phoenix Channels with full correlation.
  """
  
  def instrument_channel_module(ast, plan) do
    Macro.prewalk(ast, fn
      # Instrument join callbacks
      {:def, meta, [{:join, _, [topic, auth_msg, socket]}, body]} = join_def ->
        transformed_body = instrument_channel_join(body, topic, auth_msg, socket)
        {:def, meta, [{:join, [], [topic, auth_msg, socket]}, transformed_body]}
        
      # Instrument handle_in callbacks
      {:def, meta, [{:handle_in, _, [event, payload, socket]}, body]} = handle_in_def ->
        transformed_body = instrument_handle_in(body, event, payload, socket)
        {:def, meta, [{:handle_in, [], [event, payload, socket]}, transformed_body]}
        
      # Instrument handle_out callbacks
      {:def, meta, [{:handle_out, _, [event, payload, socket]}, body]} = handle_out_def ->
        transformed_body = instrument_handle_out(body, event, payload, socket)
        {:def, meta, [{:handle_out, [], [event, payload, socket]}, transformed_body]}
        
      node -> node
    end)
  end
  
  defp instrument_channel_join(body, topic, auth_msg, socket) do
    quote do
      correlation_id = ElixirScope.Utils.generate_correlation_id()
      
      ElixirScope.Capture.InstrumentationRuntime.report_channel_join_start(
        correlation_id,
        __MODULE__,
        unquote(topic),
        unquote(auth_msg),
        extract_socket_info(unquote(socket))
      )
      
      result = unquote(body)
      
      case result do
        {:ok, final_socket} ->
          ElixirScope.Capture.InstrumentationRuntime.report_channel_join_success(
            correlation_id,
            extract_socket_info(final_socket)
          )
          
        {:error, reason} ->
          ElixirScope.Capture.InstrumentationRuntime.report_channel_join_error(
            correlation_id,
            reason
          )
          
        other -> other
      end
      
      result
    end
  end
  
  defp instrument_handle_in(body, event, payload, socket) do
    quote do
      # Generate message correlation ID
      message_correlation_id = ElixirScope.Utils.generate_correlation_id()
      
      # Check for existing correlation from client
      client_correlation_id = extract_client_correlation_id(unquote(payload))
      
      ElixirScope.Capture.InstrumentationRuntime.report_channel_message_received(
        message_correlation_id,
        client_correlation_id,
        unquote(event),
        unquote(payload),
        extract_socket_info(unquote(socket))
      )
      
      # Execute handler with correlation context
      Process.put(:elixir_scope_correlation_id, message_correlation_id)
      
      result = unquote(body)
      
      # Report result and any outgoing messages
      case result do
        {:noreply, final_socket} ->
          ElixirScope.Capture.InstrumentationRuntime.report_channel_message_handled(
            message_correlation_id,
            :noreply,
            extract_socket_changes(unquote(socket), final_socket)
          )
          
        {:reply, reply_payload, final_socket} ->
          ElixirScope.Capture.InstrumentationRuntime.report_channel_message_handled(
            message_correlation_id,
            {:reply, reply_payload},
            extract_socket_changes(unquote(socket), final_socket)
          )
          
        {:stop, reason, final_socket} ->
          ElixirScope.Capture.InstrumentationRuntime.report_channel_message_handled(
            message_correlation_id,
            {:stop, reason},
            extract_socket_changes(unquote(socket), final_socket)
          )
          
        other -> other
      end
      
      result
    end
  end
end
```

### 7.3 AI Analysis Advanced Patterns

#### Project Architecture Analysis

```elixir
defmodule ElixirScope.AI.ArchitectureAnalyzer do
  @moduledoc """
  Analyzes overall project architecture to inform instrumentation decisions.
  """
  
  def analyze_project_architecture(project_path) do
    modules = discover_all_modules(project_path)
    
    %{
      supervision_topology: analyze_supervision_topology(modules),
      data_flow_patterns: analyze_data_flow_patterns(modules),
      communication_patterns: analyze_communication_patterns(modules),
      performance_hotspots: identify_performance_hotspots(modules),
      error_propagation_paths: trace_error_propagation_paths(modules),
      external_dependencies: analyze_external_dependencies(modules)
    }
  end
  
  defp analyze_supervision_topology(modules) do
    supervisors = find_supervisor_modules(modules)
    workers = find_worker_modules(modules)
    
    # Build supervision tree
    supervision_tree = build_supervision_tree(supervisors, workers)
    
    %{
      tree: supervision_tree,
      depth: calculate_tree_depth(supervision_tree),
      fan_out: calculate_average_fan_out(supervision_tree),
      critical_paths: identify_critical_supervision_paths(supervision_tree),
      restart_strategies: extract_restart_strategies(supervisors)
    }
  end
  
  defp analyze_data_flow_patterns(modules) do
    # Analyze how data flows through the system
    database_modules = find_database_modules(modules)
    api_modules = find_api_modules(modules)
    business_logic_modules = find_business_logic_modules(modules)
    
    data_flows = trace_data_flows(database_modules, business_logic_modules, api_modules)
    
    %{
      flows: data_flows,
      bottlenecks: identify_data_bottlenecks(data_flows),
      validation_points: find_validation_points(modules),
      transformation_points: find_transformation_points(modules),
      persistence_patterns: analyze_persistence_patterns(database_modules)
    }
  end
  
  defp analyze_communication_patterns(modules) do
    # Analyze inter-process communication patterns
    genserver_calls = extract_genserver_calls(modules)
    pubsub_patterns = extract_pubsub_patterns(modules)
    direct_messages = extract_direct_messages(modules)
    
    %{
      synchronous_calls: genserver_calls,
      asynchronous_messages: direct_messages,
      broadcast_patterns: pubsub_patterns,
      communication_density: calculate_communication_density(modules),
      coupling_analysis: analyze_module_coupling(modules)
    }
  end
  
  defp identify_performance_hotspots(modules) do
    # Use heuristics to identify likely performance bottlenecks
    computational_heavy = find_computational_heavy_modules(modules)
    io_heavy = find_io_heavy_modules(modules)
    memory_intensive = find_memory_intensive_modules(modules)
    
    %{
      computational: computational_heavy,
      io_bound: io_heavy,
      memory_intensive: memory_intensive,
      concurrency_bottlenecks: find_concurrency_bottlenecks(modules),
      database_hotspots: find_database_hotspots(modules)
    }
  end
  
  def generate_instrumentation_strategy(architecture_analysis) do
    %{
      critical_path_instrumentation: plan_critical_path_instrumentation(architecture_analysis),
      performance_monitoring: plan_performance_monitoring(architecture_analysis),
      error_tracking: plan_error_tracking(architecture_analysis),
      correlation_strategy: plan_correlation_strategy(architecture_analysis),
      sampling_strategy: plan_sampling_strategy(architecture_analysis)
    }
  end
  
  defp plan_critical_path_instrumentation(analysis) do
    critical_paths = analysis.supervision_topology.critical_paths
    hotspots = analysis.performance_hotspots
    
    # Prioritize instrumentation on critical paths and hotspots
    critical_modules = extract_critical_modules(critical_paths, hotspots)
    
    Enum.map(critical_modules, fn module ->
      %{
        module: module,
        instrumentation_level: :comprehensive,
        capture_args: true,
        capture_state: module.type in [:genserver, :agent],
        capture_performance: module in hotspots.computational,
        sampling_rate: 1.0  # Full capture for critical paths
      }
    end)
  end
  
  defp plan_performance_monitoring(analysis) do
    performance_modules = [
      analysis.performance_hotspots.computational,
      analysis.performance_hotspots.io_bound,
      analysis.performance_hotspots.memory_intensive
    ] |> List.flatten() |> Enum.uniq()
    
    Enum.map(performance_modules, fn module ->
      %{
        module: module,
        instrumentation_level: :performance_focused,
        capture_timing: true,
        capture_memory: module in analysis.performance_hotspots.memory_intensive,
        capture_io: module in analysis.performance_hotspots.io_bound,
        sampling_rate: 0.1  # Sample for performance to reduce overhead
      }
    end)
  end
  
  defp plan_correlation_strategy(analysis) do
    # Plan how to correlate events across the system
    communication_patterns = analysis.communication_patterns
    
    %{
      genserver_correlation: plan_genserver_correlation(communication_patterns.synchronous_calls),
      pubsub_correlation: plan_pubsub_correlation(communication_patterns.broadcast_patterns),
      request_correlation: plan_request_correlation(analysis.data_flow_patterns),
      error_correlation: plan_error_correlation(analysis.error_propagation_paths)
    }
  end
end
```

#### Machine Learning Integration Framework

```elixir
defmodule ElixirScope.AI.MLIntegration do
  @moduledoc """
  Framework for integrating machine learning models with ElixirScope
  for advanced pattern recognition and anomaly detection.
  """
  
  def train_instrumentation_model(historical_data) do
    # Train ML model on historical instrumentation effectiveness
    features = extract_features(historical_data)
    labels = extract_labels(historical_data)
    
    model = train_decision_tree(features, labels)
    
    %{
      model: model,
      feature_importance: calculate_feature_importance(model),
      accuracy: validate_model(model, features, labels),
      created_at: DateTime.utc_now()
    }
  end
  
  def predict_optimal_instrumentation(project_features, model) do
    # Use trained model to predict optimal instrumentation
    prediction = apply_model(model, project_features)
    confidence = calculate_prediction_confidence(model, project_features)
    
    %{
      instrumentation_plan: prediction,
      confidence: confidence,
      fallback_plan: generate_fallback_plan(project_features)
    }
  end
  
  defp extract_features(historical_data) do
    Enum.map(historical_data, fn data_point ->
      %{
        module_count: data_point.project.module_count,
        genserver_count: data_point.project.genserver_count,
        complexity_score: data_point.project.complexity_score,
        external_dependencies: length(data_point.project.dependencies),
        test_coverage: data_point.project.test_coverage,
        performance_requirements: encode_performance_requirements(data_point.requirements)
      }
    end)
  end
  
  defp extract_labels(historical_data) do
    Enum.map(historical_data, fn data_point ->
      %{
        optimal_sampling_rate: data_point.results.optimal_sampling_rate,
        best_instrumentation_level: data_point.results.best_instrumentation_level,
        performance_impact: data_point.results.performance_impact,
        bug_detection_rate: data_point.results.bug_detection_rate
      }
    end)
  end
  
  def continuous_learning_update(model, new_data) do
    # Update model with new data from production usage
    new_features = extract_features(new_data)
    new_labels = extract_labels(new_data)
    
    updated_model = incremental_train(model, new_features, new_labels)
    
    # Validate improvement
    if model_improved?(model, updated_model) do
      {:ok, updated_model}
    else
      {:unchanged, model}
    end
  end
end
```

### 7.4 Production Deployment Considerations

#### Monitoring and Observability

```elixir
defmodule ElixirScope.Production.Monitor do
  @moduledoc """
  Production monitoring for ElixirScope itself to ensure it doesn't
  impact application performance.
  """
  
  use GenServer
  
  def start_link(opts) do
    GenServer.start_link(__MODULE__, opts, name: __MODULE__)
  end
  
  def init(_opts) do
    # Schedule periodic health checks
    :timer.send_interval(30_000, :health_check)
    :timer.send_interval(60_000, :performance_check)
    :timer.send_interval(300_000, :memory_check)
    
    state = %{
      start_time: System.monotonic_time(),
      health_status: :healthy,
      performance_metrics: %{},
      memory_metrics: %{},
      alerts: []
    }
    
    {:ok, state}
  end
  
  def handle_info(:health_check, state) do
    health_status = perform_health_check()
    
    if health_status != :healthy do
      send_alert(:health_degraded, health_status)
    end
    
    {:noreply, %{state | health_status: health_status}}
  end
  
  def handle_info(:performance_check, state) do
    performance_metrics = collect_performance_metrics()
    
    # Check for performance degradation
    if performance_degraded?(performance_metrics, state.performance_metrics) do
      send_alert(:performance_degraded, performance_metrics)
      maybe_reduce_sampling(performance_metrics)
    end
    
    {:noreply, %{state | performance_metrics: performance_metrics}}
  end
  
  def handle_info(:memory_check, state) do
    memory_metrics = collect_memory_metrics()
    
    # Check for memory leaks
    if memory_leak_detected?(memory_metrics, state.memory_metrics) do
      send_alert(:memory_leak_detected, memory_metrics)
      trigger_cleanup()
    end
    
    {:noreply, %{state | memory_metrics: memory_metrics}}
  end
  
  defp perform_health_check do
    components = [
      check_ring_buffer_health(),
      check_async_workers_health(),
      check_correlation_engine_health(),
      check_storage_health()
    ]
    
    if Enum.all?(components, &(&1 == :healthy)) do
      :healthy
    else
      failed_components = 
        components
        |> Enum.with_index()
        |> Enum.reject(fn {status, _} -> status == :healthy end)
        |> Enum.map(fn {status, index} -> {component_name(index), status} end)
      
      {:degraded, failed_components}
    end
  end
  
  defp collect_performance_metrics do
    %{
      event_ingestion_rate: measure_event_ingestion_rate(),
      event_processing_latency: measure_event_processing_latency(),
      query_response_time: measure_query_response_time(),
      ring_buffer_utilization: measure_ring_buffer_utilization(),
      worker_pool_utilization: measure_worker_pool_utilization()
    }
  end
  
  defp performance_degraded?(current, previous) when map_size(previous) == 0 do
    false
  end
  defp performance_degraded?(current, previous) do
    # Check if any key metrics have degraded significantly
    latency_increase = (current.event_processing_latency / previous.event_processing_latency)
    query_slowdown = (current.query_response_time / previous.query_response_time)
    
    latency_increase > 1.5 or query_slowdown > 2.0
  end
  
  defp maybe_reduce_sampling(performance_metrics) do
    if performance_metrics.event_processing_latency > 1000 do  # 1ms
      # Reduce sampling rate to maintain performance
      current_rate = ElixirScope.Config.get_sampling_rate()
      new_rate = max(current_rate * 0.8, 0.1)  # Reduce by 20%, min 10%
      
      ElixirScope.Config.set_sampling_rate(new_rate)
      Logger.warning("ElixirScope: Reduced sampling rate to #{new_rate} due to performance degradation")
    end
  end
  
  defp send_alert(type, data) do
    alert = %{
      type: type,
      data: data,
      timestamp: DateTime.utc_now(),
      node: node()
    }
    
    # Send to monitoring system
    case Application.get_env(:elixir_scope, :monitoring_endpoint) do
      nil -> Logger.warning("ElixirScope alert: #{inspect(alert)}")
      endpoint -> send_to_monitoring_system(endpoint, alert)
    end
  end
end
```

#### Graceful Degradation Strategies

```elixir
defmodule ElixirScope.Production.DegradationStrategy do
  @moduledoc """
  Implements graceful degradation strategies when ElixirScope
  detects system stress or performance issues.
  """
  
  def apply_degradation_strategy(stress_level) do
    case stress_level do
      :low -> maintain_full_functionality()
      :medium -> apply_moderate_degradation()
      :high -> apply_aggressive_degradation()
      :critical -> apply_emergency_degradation()
    end
  end
  
  defp apply_moderate_degradation do
    # Reduce sampling rate and disable non-critical features
    %{
      sampling_rate: 0.5,
      disable_features: [:detailed_state_capture, :argument_capture],
      buffer_size_reduction: 0.8,
      async_worker_reduction: 0.7
    }
  end
  
  defp apply_aggressive_degradation do
    # Keep only essential tracing
    %{
      sampling_rate: 0.2,
      disable_features: [
        :detailed_state_capture, 
        :argument_capture, 
        :return_value_capture,
        :performance_metrics
      ],
      buffer_size_reduction: 0.5,
      async_worker_reduction: 0.5,
      correlation_simplification: true
    }
  end
  
  defp apply_emergency_degradation do
    # Minimal tracing - errors and critical events only
    %{
      sampling_rate: 0.05,
      trace_only: [:errors, :exceptions, :critical_events],
      disable_features: :all_except_critical,
      buffer_size_reduction: 0.3,
      async_worker_reduction: 0.3,
      emergency_mode: true
    }
  end
  
  def monitor_and_adjust_degradation do
    stress_level = assess_system_stress()
    
    if stress_level != get_current_stress_level() do
      new_strategy = apply_degradation_strategy(stress_level)
      implement_strategy(new_strategy)
      
      Logger.info("ElixirScope: Adjusted degradation strategy to #{stress_level}")
    end
  end
  
  defp assess_system_stress do
    metrics = %{
      cpu_usage: get_cpu_usage(),
      memory_usage: get_memory_usage(),
      message_queue_lengths: get_message_queue_lengths(),
      gc_frequency: get_gc_frequency(),
      scheduler_utilization: get_scheduler_utilization()
    }
    
    cond do
      critical_stress?(metrics) -> :critical
      high_stress?(metrics) -> :high
      medium_stress?(metrics) -> :medium
      true -> :low
    end
  end
  
  defp critical_stress?(metrics) do
    metrics.cpu_usage > 0.95 or
    metrics.memory_usage > 0.9 or
    Enum.any?(metrics.message_queue_lengths, &(&1 > 10000))
  end
  
  defp high_stress?(metrics) do
    metrics.cpu_usage > 0.8 or
    metrics.memory_usage > 0.75 or
    Enum.any?(metrics.message_queue_lengths, &(&1 > 5000)) or
    metrics.scheduler_utilization > 0.85
  end
  
  defp medium_stress?(metrics) do
    metrics.cpu_usage > 0.6 or
    metrics.memory_usage > 0.6 or
    Enum.any?(metrics.message_queue_lengths, &(&1 > 1000))
  end
  
  defp implement_strategy(strategy) do
    # Apply sampling rate changes
    if Map.has_key?(strategy, :sampling_rate) do
      ElixirScope.Config.set_sampling_rate(strategy.sampling_rate)
    end
    
    # Disable features
    if Map.has_key?(strategy, :disable_features) do
      ElixirScope.Config.disable_features(strategy.disable_features)
    end
    
    # Adjust buffer sizes
    if Map.has_key?(strategy, :buffer_size_reduction) do
      ElixirScope.Capture.RingBuffer.resize_buffers(strategy.buffer_size_reduction)
    end
    
    # Scale worker pool
    if Map.has_key?(strategy, :async_worker_reduction) do
      ElixirScope.Capture.AsyncWriterPool.scale_workers(strategy.async_worker_reduction)
    end
    
    # Simplify correlation if needed
    if Map.get(strategy, :correlation_simplification, false) do
      ElixirScope.Capture.EventCorrelator.enable_simple_mode()
    end
    
    # Emergency mode
    if Map.get(strategy, :emergency_mode, false) do
      enable_emergency_mode(strategy)
    end
  end
  
  defp enable_emergency_mode(strategy) do
    # Disable all non-essential components
    ElixirScope.Phoenix.Integration.disable_non_critical()
    ElixirScope.AI.Orchestrator.pause_analysis()
    
    # Only trace specified event types
    if Map.has_key?(strategy, :trace_only) do
      ElixirScope.Capture.InstrumentationRuntime.set_trace_filter(strategy.trace_only)
    end
    
    Logger.warning("ElixirScope: Entered emergency mode - minimal tracing only")
  end
end
```

### 7.5 Testing Framework Enhancements

#### Chaos Engineering for ElixirScope

```elixir
defmodule ElixirScope.Testing.ChaosEngineering do
  @moduledoc """
  Chaos engineering tests to validate ElixirScope's resilience
  under various failure conditions.
  """
  
  def run_chaos_test_suite do
    chaos_scenarios = [
      :ring_buffer_corruption,
      :async_worker_cascade_failure,
      :correlation_engine_deadlock,
      :storage_disk_full,
      :network_partition,
      :memory_pressure,
      :high_cpu_contention,
      :process_thrashing
    ]
    
    results = for scenario <- chaos_scenarios do
      run_chaos_scenario(scenario)
    end
    
    analyze_chaos_results(results)
  end
  
  def run_chaos_scenario(scenario) do
    # Start with clean state
    reset_elixir_scope()
    
    # Generate baseline load
    baseline_task = Task.async(fn -> generate_steady_load(60_000) end)
    
    # Wait for steady state
    Process.sleep(5_000)
    
    # Inject chaos
    chaos_result = inject_chaos(scenario)
    
    # Monitor system behavior
    monitoring_task = Task.async(fn -> monitor_system_during_chaos(60_000) end)
    
    # Wait for test completion
    baseline_result = Task.await(baseline_task, 120_000)
    monitoring_result = Task.await(monitoring_task, 120_000)
    
    # Analyze results
    %{
      scenario: scenario,
      chaos_injection: chaos_result,
      system_behavior: monitoring_result,
      baseline_performance: baseline_result,
      recovery_metrics: measure_recovery_metrics(),
      data_integrity: verify_data_integrity()
    }
  end
  
  defp inject_chaos(:ring_buffer_corruption) do
    # Corrupt ring buffer data structures
    ring_buffers = ElixirScope.Capture.RingBuffer.get_all_buffers()
    
    Enum.each(ring_buffers, fn buffer ->
      # Simulate corruption by writing invalid data
      spawn(fn ->
        for _i <- 1..100 do
          ElixirScope.Capture.RingBuffer.write(buffer, :invalid_event_data)
          Process.sleep(10)
        end
      end)
    end)
    
    %{type: :ring_buffer_corruption, injected_at: System.monotonic_time()}
  end
  
  defp inject_chaos(:async_worker_cascade_failure) do
    # Kill async workers in cascade
    workers = ElixirScope.Capture.AsyncWriterPool.get_all_workers()
    
    # Kill workers one by one with delays
    spawn(fn ->
      Enum.with_index(workers)
      |> Enum.each(fn {worker, index} ->
        Process.sleep(index * 1000)  # Stagger the failures
        Process.exit(worker, :chaos_kill)
      end)
    end)
    
    %{type: :cascade_failure, workers_killed: length(workers)}
  end
  
  defp inject_chaos(:correlation_engine_deadlock) do
    # Create conditions that could cause deadlock
    correlation_engine = Process.whereis(ElixirScope.Capture.EventCorrelator)
    
    # Send conflicting messages simultaneously
    spawn(fn ->
      for _i <- 1..100 do
        send(correlation_engine, {:fake_event, :high_priority})
        send(correlation_engine, {:fake_event, :low_priority})
        Process.sleep(1)
      end
    end)
    
    %{type: :potential_deadlock, messages_sent: 200}
  end
  
  defp inject_chaos(:memory_pressure) do
    # Create memory pressure by allocating large chunks
    memory_pressure_process = spawn(fn ->
      accumulator = for _i <- 1..1000 do
        # Allocate 1MB chunks
        :binary.copy(<<0>>, 1024 * 1024)
      end
      
      # Hold memory for test duration
      Process.sleep(60_000)
      
      # Clean up
      accumulator = nil
      :erlang.garbage_collect()
    end)
    
    %{type: :memory_pressure, pressure_process: memory_pressure_process}
  end
  
  defp monitor_system_during_chaos(duration_ms) do
    end_time = System.monotonic_time(:millisecond) + duration_ms
    
    measurements = Stream.unfold(System.monotonic_time(:millisecond), fn current_time ->
      if current_time < end_time do
        measurement = %{
          timestamp: current_time,
          event_processing_rate: measure_event_processing_rate(),
          ring_buffer_health: check_ring_buffer_health(),
          worker_pool_status: check_worker_pool_status(),
          correlation_engine_status: check_correlation_engine_status(),
          memory_usage: measure_elixir_scope_memory(),
          cpu_usage: measure_elixir_scope_cpu(),
          error_rate: measure_error_rate()
        }
        
        Process.sleep(1000)  # Measure every second
        {measurement, System.monotonic_time(:millisecond)}
      else
        nil
      end
    end)
    |> Enum.to_list()
    
    %{
      measurements: measurements,
      summary: summarize_measurements(measurements)
    }
  end
  
  defp measure_recovery_metrics do
    # Measure how quickly the system recovers after chaos
    start_time = System.monotonic_time(:millisecond)
    
    # Wait for system to stabilize
    recovery_complete = wait_for_recovery()
    
    end_time = System.monotonic_time(:millisecond)
    
    %{
      recovery_time_ms: end_time - start_time,
      recovery_successful: recovery_complete,
      final_system_state: get_system_state()
    }
  end
  
  defp wait_for_recovery do
    # Wait up to 5 minutes for recovery
    timeout = System.monotonic_time(:millisecond) + 300_000
    
    Stream.repeatedly(fn ->
      if System.monotonic_time(:millisecond) < timeout do
        if system_recovered? do
          :recovered
        else
          Process.sleep(1000)
          :checking
        end
      else
        :timeout
      end
    end)
    |> Stream.take_while(&(&1 == :checking))
    |> Stream.run()
    
    system_recovered?()
  end
  
  defp system_recovered? do
    health_check = ElixirScope.Production.Monitor.perform_health_check()
    event_rate = measure_event_processing_rate()
    error_rate = measure_error_rate()
    
    health_check == :healthy and
    event_rate > 100 and  # Processing at least 100 events/sec
    error_rate < 0.01     # Less than 1% error rate
  end
  
  defp verify_data_integrity do
    # Verify that data wasn't corrupted during chaos
    sample_correlation_ids = get_sample_correlation_ids(100)
    
    integrity_results = Enum.map(sample_correlation_ids, fn correlation_id ->
      events = ElixirScope.Storage.DataAccess.get_events_by_correlation(correlation_id)
      
      %{
        correlation_id: correlation_id,
        event_count: length(events),
        events_valid: Enum.all?(events, &valid_event?/1),
        correlation_intact: correlation_intact?(events),
        timestamp_ordering: timestamp_ordering_correct?(events)
      }
    end)
    
    %{
      total_samples: length(integrity_results),
      valid_samples: Enum.count(integrity_results, &all_checks_passed?/1),
      integrity_percentage: calculate_integrity_percentage(integrity_results)
    }
  end
  
  defp valid_event?(event) do
    event.id != nil and
    event.timestamp != nil and
    event.event_type != nil and
    is_pid(event.pid)
  end
  
  defp correlation_intact?(events) do
    correlation_ids = Enum.map(events, & &1.correlation_id) |> Enum.uniq()
    length(correlation_ids) <= 1
  end
  
  defp timestamp_ordering_correct?(events) do
    timestamps = Enum.map(events, & &1.timestamp)
    timestamps == Enum.sort(timestamps)
  end
end
```

#### Performance Regression Detection

```elixir
defmodule ElixirScope.Testing.RegressionDetection do
  @moduledoc """
  Automated performance regression detection for ElixirScope.
  """
  
  def run_regression_suite do
    baseline_metrics = load_baseline_metrics()
    current_metrics = collect_current_metrics()
    
    regressions = detect_regressions(baseline_metrics, current_metrics)
    
    if length(regressions) > 0 do
      {:regressions_detected, regressions}
    else
      {:no_regressions, current_metrics}
    end
  end
  
  defp collect_current_metrics do
    %{
      event_ingestion: benchmark_event_ingestion(),
      event_processing: benchmark_event_processing(),
      query_performance: benchmark_query_performance(),
      memory_usage: benchmark_memory_usage(),
      correlation_performance: benchmark_correlation_performance(),
      phoenix_integration_overhead: benchmark_phoenix_overhead()
    }
  end
  
  defp benchmark_event_ingestion do
    # Benchmark event ingestion throughput
    buffer = ElixirScope.Capture.RingBuffer.new()
    event = create_benchmark_event()
    
    {time_microseconds, :ok} = :timer.tc(fn ->
      for _i <- 1..10_000 do
        ElixirScope.Capture.RingBuffer.write(buffer, event)
      end
    end)
    
    %{
      events_per_second: 10_000 / (time_microseconds / 1_000_000),
      average_latency_ns: (time_microseconds * 1000) / 10_000,
      memory_overhead: measure_buffer_memory_overhead(buffer)
    }
  end
  
  defp benchmark_event_processing do
    # Benchmark async event processing
    events = for _i <- 1..1_000, do: create_benchmark_event()
    
    {time_microseconds, processed_events} = :timer.tc(fn ->
      ElixirScope.Capture.AsyncWriterPool.process_events_sync(events)
    end)
    
    %{
      processing_rate: length(processed_events) / (time_microseconds / 1_000_000),
      average_processing_time: time_microseconds / length(events),
      correlation_success_rate: calculate_correlation_success_rate(processed_events)
    }
  end
  
  defp benchmark_query_performance do
    # Benchmark query performance with various query types
    setup_benchmark_data(10_000)
    
    query_benchmarks = %{
      simple_correlation_query: benchmark_query(fn ->
        ElixirScope.Storage.DataAccess.get_events_by_correlation("test_correlation_1")
      end),
      
      time_range_query: benchmark_query(fn ->
        start_time = System.monotonic_time() - 3_600_000_000_000  # 1 hour ago
        end_time = System.monotonic_time()
        ElixirScope.Storage.DataAccess.query_by_time_range(start_time, end_time)
      end),
      
      process_query: benchmark_query(fn ->
        test_pid = self()
        ElixirScope.Storage.DataAccess.query_by_process(test_pid)
      end),
      
      complex_correlation_query: benchmark_query(fn ->
        ElixirScope.Storage.QueryCoordinator.get_correlation_chain("test_correlation_1")
      end)
    }
    
    query_benchmarks
  end
  
  defp benchmark_phoenix_overhead do
    # Benchmark Phoenix integration overhead
    without_elixir_scope = benchmark_phoenix_request(instrumentation: false)
    with_elixir_scope = benchmark_phoenix_request(instrumentation: true)
    
    %{
      baseline_latency: without_elixir_scope.average_latency,
      instrumented_latency: with_elixir_scope.average_latency,
      overhead_percentage: calculate_overhead_percentage(
        with_elixir_scope.average_latency,
        without_elixir_scope.average_latency
      ),
      throughput_impact: calculate_throughput_impact(
        with_elixir_scope.throughput,
        without_elixir_scope.throughput
      )
    }
  end
  
  defp detect_regressions(baseline, current) do
    regression_thresholds = %{
      event_ingestion_degradation: 0.15,      # 15% slower is a regression
      memory_usage_increase: 0.20,            # 20% more memory is a regression
      query_performance_degradation: 0.25,    # 25% slower queries is a regression
      phoenix_overhead_increase: 0.10         # 10% more overhead is a regression
    }
    
    regressions = []
    
    # Check event ingestion performance
    ingestion_degradation = calculate_degradation(
      current.event_ingestion.events_per_second,
      baseline.event_ingestion.events_per_second
    )
    
    regressions = if ingestion_degradation > regression_thresholds.event_ingestion_degradation do
      [%{
        type: :event_ingestion_regression,
        degradation: ingestion_degradation,
        current: current.event_ingestion.events_per_second,
        baseline: baseline.event_ingestion.events_per_second
      } | regressions]
    else
      regressions
    end
    
    # Check memory usage
    memory_increase = calculate_increase(
      current.memory_usage.total_memory,
      baseline.memory_usage.total_memory
    )
    
    regressions = if memory_increase > regression_thresholds.memory_usage_increase do
      [%{
        type: :memory_usage_regression,
        increase: memory_increase,
        current: current.memory_usage.total_memory,
        baseline: baseline.memory_usage.total_memory
      } | regressions]
    else
      regressions
    end
    
    # Check query performance
    query_degradation = calculate_average_degradation(
      current.query_performance,
      baseline.query_performance
    )
    
    regressions = if query_degradation > regression_thresholds.query_performance_degradation do
      [%{
        type: :query_performance_regression,
        degradation: query_degradation,
        details: compare_query_performance(current.query_performance, baseline.query_performance)
      } | regressions]
    else
      regressions
    end
    
    # Check Phoenix overhead
    overhead_increase = calculate_increase(
      current.phoenix_integration_overhead.overhead_percentage,
      baseline.phoenix_integration_overhead.overhead_percentage
    )
    
    regressions = if overhead_increase > regression_thresholds.phoenix_overhead_increase do
      [%{
        type: :phoenix_overhead_regression,
        increase: overhead_increase,
        current: current.phoenix_integration_overhead.overhead_percentage,
        baseline: baseline.phoenix_integration_overhead.overhead_percentage
      } | regressions]
    else
      regressions
    end
    
    regressions
  end
  
  defp calculate_degradation(current_value, baseline_value) do
    if baseline_value > 0 do
      (baseline_value - current_value) / baseline_value
    else
      0
    end
  end
  
  defp calculate_increase(current_value, baseline_value) do
    if baseline_value > 0 do
      (current_value - baseline_value) / baseline_value
    else
      0
    end
  end
end
```

---

## 8. Final Implementation Checklist and Success Criteria

### 8.1 Phase Completion Checklist

#### Phase 1: AST Transformation Foundation  Ready for Implementation
- [ ] **Core Infrastructure**
  - [ ] `ElixirScope.Compiler.MixTask` with Mix compiler integration
  - [ ] `ElixirScope.AST.Transformer` with function instrumentation
  - [ ] `ElixirScope.AST.InjectorHelpers` with code generation patterns
  - [ ] Exception handling and error recovery

- [ ] **Language Support**
  - [ ] Basic function definitions (def, defp)
  - [ ] Functions with guards and multiple clauses
  - [ ] GenServer callbacks with state capture
  - [ ] Common macro patterns (use, import, alias)

- [ ] **Testing Requirements**
  - [ ] Semantic equivalence tests (95% pass rate)
  - [ ] Performance overhead tests (<5% impact)
  - [ ] Edge case handling (complex patterns, nested functions)
  - [ ] Integration with real Elixir projects

#### Phase 2: Phoenix Integration Layer  Ready for Implementation
- [ ] **HTTP Lifecycle**
  - [ ] Request/response correlation via Telemetry
  - [ ] Controller action instrumentation
  - [ ] Plug pipeline tracing
  - [ ] Error and exception handling

- [ ] **Real-time Features**
  - [ ] LiveView mount, event, and state tracking
  - [ ] Channel join and message flow correlation
  - [ ] PubSub broadcast/receive tracking
  - [ ] WebSocket message correlation

- [ ] **Testing Requirements**
  - [ ] Complete request lifecycle tracing (90% coverage)
  - [ ] LiveView state change correlation (95% accuracy)
  - [ ] Performance impact measurement (<10% latency increase)
  - [ ] Integration with existing Phoenix applications

#### Phase 3: AI Code Analysis Engine  Ready for Implementation
- [ ] **Pattern Recognition**
  - [ ] OTP pattern identification (GenServer, Supervisor, etc.)
  - [ ] Phoenix pattern recognition (Controller, LiveView, Channel)
  - [ ] Complexity analysis and performance prediction
  - [ ] Project architecture analysis

- [ ] **Instrumentation Planning**
  - [ ] Rule-based instrumentation strategies
  - [ ] Performance impact estimation
  - [ ] Priority-based module selection
  - [ ] Adaptive sampling strategies

- [ ] **Testing Requirements**
  - [ ] Pattern recognition accuracy (90%+ on test projects)
  - [ ] Instrumentation plan effectiveness validation
  - [ ] Performance impact prediction accuracy (20%)
  - [ ] Cross-project generalization testing

#### Phase 4: Distributed System Support  Ready for Implementation
- [ ] **Multi-Node Coordination**
  - [ ] Node discovery and cluster formation
  - [ ] Cross-node event correlation
  - [ ] Distributed correlation ID management
  - [ ] Network partition handling

- [ ] **Event Synchronization**
  - [ ] Efficient delta synchronization
  - [ ] Conflict resolution strategies
  - [ ] Bandwidth optimization
  - [ ] Eventual consistency guarantees

- [ ] **Testing Requirements**
  - [ ] Cross-node correlation accuracy (95%+ success rate)
  - [ ] Network partition recovery testing
  - [ ] Performance impact measurement (<1% network overhead)
  - [ ] Multi-node scalability testing (5+ nodes)

### 8.2 Success Criteria Matrix

| Phase | Functional Success | Performance Success | Integration Success | Quality Success |
|-------|-------------------|-------------------|-------------------|-----------------|
| **Phase 1** | 95% language constructs supported | <5% runtime overhead | Works with Mix build | 90%+ test coverage |
| **Phase 2** | 90% Phoenix features traced | <10% request latency impact | Zero Phoenix app changes | Real app validation |
| **Phase 3** | 90% pattern recognition accuracy | Analysis <5min for typical project | Integrates with existing workflow | Cross-project validation |
| **Phase 4** | 95% cross-node correlation | <1% network overhead | Transparent multi-node operation | Partition resilience |

### 8.3 Risk Mitigation Status

#### High-Risk Items - Mitigated 
1. **AST Transformation Complexity**
   - *Mitigation*: Incremental implementation starting with simple cases
   - *Status*: Test suite covers edge cases, fallback strategies defined

2. **Performance Impact**
   - *Mitigation*: Continuous performance monitoring, adaptive sampling
   - *Status*: Performance targets defined, regression detection automated

3. **Phoenix Integration Compatibility**
   - *Mitigation*: Use stable Telemetry events, version compatibility matrix
   - *Status*: Multi-version testing planned, upgrade path defined

#### Medium-Risk Items - Monitored 
1. **AI Analysis Accuracy**
   - *Mitigation*: Extensive test suite, user feedback integration
   - *Status*: Ground truth datasets prepared, accuracy metrics defined

2. **Distributed System Complexity**
   - *Mitigation*: Robust testing, graceful degradation modes
   - *Status*: Chaos engineering suite prepared, fallback modes implemented

### 8.4 Documentation Requirements

#### Developer Documentation
- [ ] **Quick Start Guide** - 15-minute setup for new users
- [ ] **Integration Guide** - Phoenix/LiveView specific instructions
- [ ] **Configuration Reference** - Complete configuration options
- [ ] **Troubleshooting Guide** - Common issues and solutions
- [ ] **Performance Tuning Guide** - Optimization recommendations

#### Technical Documentation
- [ ] **Architecture Overview** - System design and component interaction
- [ ] **API Reference** - Complete API documentation with examples
- [ ] **Extension Guide** - How to extend ElixirScope functionality
- [ ] **Contributor Guide** - Development setup and contribution workflow

#### Operational Documentation
- [ ] **Deployment Guide** - Production deployment recommendations
- [ ] **Monitoring Guide** - Setting up ElixirScope monitoring
- [ ] **Upgrade Guide** - Version migration instructions
- [ ] **Security Guide** - Security considerations and best practices

### 8.5 Final Validation Criteria

The ElixirScope foundation implementation is considered successful when:

1. **All Phase 1-4 components are implemented and tested** with success criteria met
2. **Performance targets are achieved** across all phases with regression detection in place
3. **Integration testing passes** with multiple real-world Phoenix applications
4. **Documentation is complete** and validated by external users
5. **Production readiness is validated** through chaos engineering and load testing
6. **User acceptance criteria are met** through beta testing and feedback incorporation

### 8.6 Post-Implementation Roadmap

#### Immediate Next Steps (Weeks 21-24)
- **Beta Testing Program**: Deploy with select Phoenix applications
- **Performance Optimization**: Fine-tune based on real-world usage
- **Documentation Polish**: Complete user-facing documentation
- **Community Engagement**: Gather feedback from Elixir community

#### Medium-term Goals (Months 7-12)
- **Advanced AI Integration**: Replace rule-based AI with ML models
- **UI/UX Development**: Build the "Execution Cinema" visualization interface
- **Enterprise Features**: Add role-based access, audit logging, compliance features
- **Ecosystem Integration**: Build integrations with popular Elixir tools

#### Long-term Vision (Year 2+)
- **Self-Healing Systems**: AI-driven automatic issue resolution
- **Predictive Analysis**: Machine learning-based anomaly prediction
- **Collaborative Debugging**: Team-based debugging and knowledge sharing
- **Cloud Service**: Hosted ElixirScope service for easy adoption

---

## Conclusion

This implementation guide provides a comprehensive roadmap for building the missing foundational components of ElixirScope. The four-phase approach ensures incremental value delivery while building toward the ambitious "AI-Powered Execution Cinema" vision.

**Key Success Factors:**
1. **Start with AST Transformation** - This bridges infrastructure and user value
2. **Validate with Real Applications** - Phoenix integration proves real-world value  
3. **Build AI Incrementally** - Start with rules, evolve to machine learning
4. **Test Rigorously** - Comprehensive testing ensures production readiness

**Expected Outcomes:**
- **Phase 1**: Automatic instrumentation working with any Elixir application
- **Phase 2**: Complete Phoenix request lifecycle tracing with minimal performance impact
- **Phase 3**: AI-driven instrumentation plans that outperform manual configuration
- **Phase 4**: Distributed tracing across multi-node Elixir clusters

The implementation prioritizes delivering immediate value (AST transformation) while building toward the long-term vision of AI-powered, visual debugging. Each phase has clear success criteria, comprehensive testing requirements, and risk mitigation strategies.

This foundation will enable ElixirScope to transition from an excellent infrastructure prototype to a revolutionary debugging tool that fundamentally changes how developers understand and debug concurrent Elixir applications.